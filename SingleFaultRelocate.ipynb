{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan Mayen\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import operator\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque, namedtuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nlopt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from obspy import read\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy.io.sac.util import SacIOError\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "from pyproj import Geod\n",
    "from scipy.optimize import curve_fit, least_squares, minimize\n",
    "\n",
    "from GetData import AbstractFaultProcess\n",
    "from utils import *\n",
    "\n",
    "def cc(fa, id1, t1, lat1, lon1, mag1, id2, t2, lat2, lon2, mag2):\n",
    "    # cannot serialize pandas frame, which sliently quits multiprocessor\n",
    "    print(f\"Processing pair: {id1}-{id2}\")\n",
    "\n",
    "    outlierCC = fa.relocationConfig.get(\"outlierCC\", 0.70)\n",
    "    outlierDt = fa.relocationConfig.get(\"outlierDt\", 35)\n",
    "    numCCToFit = fa.relocationConfig.get(\"numCCToFit\", 8)\n",
    "    vgroup = fa.relocationConfig.get(\"vgroup\", 3.75)\n",
    "    outlierDx = vgroup * outlierDt\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "    def badCoverage(azi, maxgap=120.0):  # exclude those with azimuth gap larger than `maxgap`\n",
    "        if len(azi) == 0:\n",
    "            return False\n",
    "        _azi = sorted([x % 360 for x in azi])\n",
    "        diff = [_azi[i+1] - _azi[i] for i in range(len(_azi) - 1)]\n",
    "        diff.append(360 - _azi[-1] + _azi[0])\n",
    "        return np.any(np.array(diff) > maxgap)\n",
    "\n",
    "    def residual_cosine(x, azi, dt):\n",
    "        return x[0] + x[1] * np.cos(np.deg2rad(azi - x[2])) - dt\n",
    "\n",
    "    def fit_cosine_ls_norm(azi, dx, cc, u0):\n",
    "        mask = np.logical_and(cc >= outlierCC, np.abs(dx) <= outlierDx)\n",
    "        _azi = azi[mask]\n",
    "        _dx = dx[mask]\n",
    "        if len(_azi) < numCCToFit or badCoverage(_azi):\n",
    "            return np.array([np.nan, np.nan, np.nan]), np.array([np.inf, np.inf, np.inf])\n",
    "\n",
    "        # initial solution affect the local minimum obtained\n",
    "        res = least_squares(residual_cosine, [0., u0, 0.], args=(\n",
    "            _azi, _dx), loss='soft_l1', jac='3-point')\n",
    "\n",
    "        # On way of uncertainty, but inappropriate since we use other forms of norm\n",
    "        # https://stackoverflow.com/a/21844726/8697614\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # pcov = cov * (res.fun ** 2).sum() / (len(_azi) - len(res.x)) # reduced chi-square distribution\n",
    "        # err = np.sqrt(np.diagonal(pcov))\n",
    "\n",
    "        # Another way of uncertainty, still assume your function value is chi-square\n",
    "        # https://stackoverflow.com/a/53489234/8697614, notice `dx[2] can exceed 360`\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # err2 = np.sqrt(np.diagonal(cov) * np.abs(res.cost))\n",
    "        # return res.x, err\n",
    "\n",
    "        # The third way: bootstrap\n",
    "        # This is what McGuire did, assuming 1s error in dt\n",
    "        # err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=1.0 * vgroup)\n",
    "        # we instead use fitting residual as the error for bootstrap\n",
    "        err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=np.std(\n",
    "            residual_cosine(res.x, _azi, _dx)))\n",
    "        return res.x, err\n",
    "\n",
    "    def bootstrap_uncertainty(x, y, p0, numiters=400, stderr=3.75, nsigma=1.0):\n",
    "        x_cmt, x_dist, x_azi = [], [], []\n",
    "        n = len(y)\n",
    "        for i in range(numiters):\n",
    "            yy = y + np.random.default_rng().normal(0.0, stderr, n)\n",
    "            res = least_squares(residual_cosine, p0, args=(\n",
    "                x, yy), loss='soft_l1', jac='3-point')\n",
    "            x_cmt.append(res.x[0])\n",
    "            x_dist.append(res.x[1])\n",
    "            x_azi.append(res.x[2])\n",
    "        return nsigma*np.std(x_cmt), nsigma*np.std(x_dist), nsigma*np.std(x_azi)\n",
    "\n",
    "    fm1, fm2 = fa.fm[str(id1)][\"np1\"], fa.fm[str(id2)][\"np1\"]\n",
    "    wv1, wv2 = [], []\n",
    "    content = {x: [] for x in [\"azi\", \"dt\",\n",
    "                               \"cc\", \"net\", \"sta\", \"staLat\", \"staLon\"]}\n",
    "    nameBase = os.path.join(\n",
    "        fa.ccDir, '-'.join(map(str, [id1, id2])))\n",
    "    dfStations = pd.read_csv(os.path.join(fa.dir, \"stations.csv\"))\n",
    "    for rs in dfStations.itertuples():\n",
    "        waveName1 = str(id1) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        waveName2 = str(id2) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        wavePath1 = os.path.join(fa.waveDir, waveName1)\n",
    "        wavePath2 = os.path.join(fa.waveDir, waveName2)\n",
    "\n",
    "        if (\n",
    "            np.isnan(rs.dist) or\n",
    "            not os.path.isfile(wavePath1) or\n",
    "            not os.path.isfile(wavePath2) or\n",
    "            os.path.getsize(wavePath1) == 0 or\n",
    "            os.path.getsize(wavePath2) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        midLon, midLat = angularMean(\n",
    "            [lon1, lon2]), angularMean([lat1, lat2])\n",
    "        fz, _, _ = geod.inv(midLon, midLat, rs.lon,\n",
    "                            rs.lat)  # fz: (-180, 180)\n",
    "        try:\n",
    "            st1 = read(wavePath1, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath1)\n",
    "            continue\n",
    "        try:\n",
    "            st2 = read(wavePath2, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath2)\n",
    "            continue\n",
    "\n",
    "        if st1[0].stats[\"sampling_rate\"] != st2[0].stats[\"sampling_rate\"]:\n",
    "            maxSamplingRate = max(st1[0].stats[\"sampling_rate\"], st2[0].stats[\"sampling_rate\"])\n",
    "            for ss in [st1, st2]:\n",
    "                if maxSamplingRate != ss[0].stats[\"sampling_rate\"]:\n",
    "                    ss.resample(maxSamplingRate, no_filter=True)\n",
    "\n",
    "        cc = correlate(\n",
    "            st1[0], st2[0], demean=True, normalize='naive',\n",
    "            shift=min(st1[0].stats.npts, st2[0].stats.npts, int(\n",
    "                np.round(outlierDt / st1[0].stats.delta * 2.0)))\n",
    "        )\n",
    "        # consider only positive cc\n",
    "        shift, value = xcorr_max(cc, abs_max=False)\n",
    "        dt = st1[0].stats.delta * shift\n",
    "        content['azi'].append(fz)\n",
    "        content['dt'].append(dt)\n",
    "        content['cc'].append(value)\n",
    "        content['net'].append(rs.net)\n",
    "        content['sta'].append(rs.sta)\n",
    "        content['staLat'].append(rs.lat)\n",
    "        content['staLon'].append(rs.lon)\n",
    "        wv1.append(st1[0].data)\n",
    "        wv2.append(st2[0].data)\n",
    "\n",
    "    pd.DataFrame(content).to_csv(nameBase + \".csv\", index=False)\n",
    "    if len(content[\"dt\"]) < numCCToFit:\n",
    "        return\n",
    "\n",
    "    yy = np.array(content['dt']) * vgroup\n",
    "    xx = np.array(content['azi'])\n",
    "    # initial guess of distance, important for least square fit\n",
    "    _, _, u0 = geod.inv(lon1, lat1, lon2, lat2)\n",
    "    popt, pcov = fit_cosine_ls_norm(\n",
    "        xx, yy, np.array(content['cc']), np.abs(u0 / 1e3))\n",
    "    A, B, C = popt\n",
    "    dA, dB, dC = pcov\n",
    "    highcc = list(filter(lambda x: np.abs(x) >= outlierCC, content['cc']))\n",
    "    summary = {\n",
    "        'A': popt[0], 'B': popt[1], 'C': popt[2],\n",
    "        'dA': pcov[0], 'dB': pcov[1], 'dC': pcov[2],\n",
    "        'numHighCC': len(highcc),\n",
    "    }\n",
    "    with open(nameBase + \".json\", \"w\") as fp:\n",
    "        json.dump(summary, fp, indent=4)\n",
    "\n",
    "    def plotCosineFit():\n",
    "        fig = plt.figure(figsize=(8, 10))\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[2, 1])\n",
    "        ax0 = fig.add_subplot(gs[0, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(\n",
    "            gs[1, 0], projection=ccrs.AzimuthalEquidistant(lon1, lat1))\n",
    "        ax2.stock_img()\n",
    "        ax2.scatter(lon1, lat1, s=20, marker=\"*\", c=\"orange\",\n",
    "                    transform=ccrs.PlateCarree(), zorder=2)\n",
    "\n",
    "        ax3 = fig.add_axes([0.55, 0.89, 0.1, 0.1])\n",
    "        ax3.set_aspect(\"equal\")\n",
    "        if np.isnan(fm1[0]):\n",
    "            bc1 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.75), width=0.4)\n",
    "        else:\n",
    "            bc1 = beach(fm1, facecolor=\"tab:blue\", xy=(0.5, 0.75), width=0.4)\n",
    "        if np.isnan(fm2[0]):\n",
    "            bc2 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.25), width=0.4)\n",
    "        else:\n",
    "            bc2 = beach(fm2, facecolor=\"tab:red\", xy=(0.5, 0.25), width=0.4)\n",
    "        ax3.add_collection(bc1)\n",
    "        ax3.add_collection(bc2)\n",
    "        ax3.axis('off')\n",
    "\n",
    "        for i in range(len(wv1)):\n",
    "            # do not set `step=delta` which may cause length mismatch\n",
    "            y1 = np.arange(0.0, len(wv1[i])) * st1[0].stats.delta\n",
    "            y2 = np.arange(0.0, len(wv2[i])) * st2[0].stats.delta\n",
    "            w1 = wv1[i] - np.nanmean(wv1[i])\n",
    "            w2 = wv2[i] - np.nanmean(wv2[i])\n",
    "            w1 = w1 / np.nanmax(np.abs(w1)) * 8\n",
    "            w2 = w2 / np.nanmax(np.abs(w2)) * 8\n",
    "\n",
    "            isoutlier = not (np.abs(content[\"dt\"][i]) <= outlierDt and np.abs(\n",
    "                content[\"cc\"][i]) >= outlierCC)\n",
    "            linestyle = '--' if isoutlier else '-'\n",
    "            alpha = 0.2 if isoutlier else 1.0\n",
    "            fc = 'white' if isoutlier else 'tab:green'\n",
    "            ax0.plot(y1, w1 + content[\"azi\"][i], color=\"tab:blue\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax0.plot(y2, w2 + content[\"azi\"][i], color=\"tab:red\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax1.scatter(content[\"dt\"][i], content[\"azi\"][i],\n",
    "                        fc=fc, s=15, marker=\"o\", ec=\"black\", zorder=2)\n",
    "\n",
    "            trajectorColor = \"gray\" if isoutlier else \"tab:pink\"\n",
    "            trajectorStyle = \"--\" if isoutlier else \"-\"\n",
    "            ax2.plot([lon1, content[\"staLon\"][i]], [lat1, content[\"staLat\"][i]],\n",
    "                     color=trajectorColor, transform=ccrs.Geodetic(), zorder=1, linestyle=trajectorStyle)\n",
    "\n",
    "        xx = np.arange(-180, 180, 1.0)\n",
    "        if np.isnan(A):\n",
    "            yy = np.array([np.nan for _ in xx])\n",
    "            fittingColor = \"gray\"\n",
    "        else:\n",
    "            yy = A + B * np.cos(np.deg2rad(xx - C))\n",
    "            yy /= vgroup\n",
    "            fittingColor = \"tab:orange\"\n",
    "\n",
    "        ax1.plot(yy, xx, c=fittingColor, zorder=1)\n",
    "        _b, _c = B, C\n",
    "        if _b < 0:\n",
    "            _b *= -1\n",
    "            _c += 180\n",
    "        _c %= 360\n",
    "        ax1.text(0.5, 1.05, \"$\\Delta x = \\;$\" + f\"{_b:.0f}\" + \"$\\;\\pm \\;$\" + f\"{dB:.0f}\" + \"$\\;\\mathrm{km}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax1.text(0.5, 1.12, r\"$\\theta = \\;$\" + f\"{_c:.1f}\" + \"$\\;\\pm \\;$\" + f\"{dC:.1f}\" + \"$^{\\circ}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax0.set_ylim([-180, 180])\n",
    "        ax1.set_ylim([-180, 180])\n",
    "        ax1.set_xlim([-outlierDt, outlierDt])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.grid(True, which=\"major\", color='gray',\n",
    "                 linestyle='--', linewidth=1)\n",
    "        ax1.set_xlabel(\"$\\Delta T$ (second)\")\n",
    "        ax0.set_xlabel(\"Time (second)\")\n",
    "        ax0.set_ylabel(\"Azimuth (deg)\")\n",
    "        ax0.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "        l1, = ax0.plot([], [], color=\"tab:blue\")\n",
    "        l2, = ax0.plot([], [], color=\"tab:red\")\n",
    "        _tstr1 = datetime.fromisoformat(t1)\n",
    "        _tstr1 = _tstr1.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag1:.1f}')\n",
    "        _tstr2 = datetime.fromisoformat(t2)\n",
    "        _tstr2 = _tstr2.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag2:.1f}')\n",
    "        ax0.legend([l1, l2], [_tstr1, _tstr2],\n",
    "                   loc=\"upper center\", bbox_to_anchor=(0.5, 1.18))\n",
    "\n",
    "        l1, = ax1.plot([], [], color=\"tab:orange\")\n",
    "        l2 = ax0.scatter([], [], fc='tab:green', s=15, marker=\"o\", ec=\"black\")\n",
    "        l3 = ax0.scatter([], [], fc='white', s=15, marker=\"o\", ec=\"black\")\n",
    "        ax1.legend([l1, l2, l3], [\"Cosine Fitting\", \"Observation (cc $\\geq \\;$\" +\n",
    "                                  f\"{outlierCC:.2f})\", f\"Observation (cc $<$ {outlierCC:.2f})\"], loc=\"lower center\", bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "        l1, = ax2.plot([], [], color=\"tab:pink\", linestyle=\"-\")\n",
    "        l2, = ax2.plot([], [], color=\"gray\", linestyle=\"--\")\n",
    "        l3 = ax2.scatter([], [], s=20, marker=\"*\", c=\"orange\")\n",
    "        ax2.legend([l1, l2, l3], [\"$\\mathrm{cc} \\geq \\;$\"+f\"{outlierCC:.2f}\", \"$\\mathrm{cc} <\\;$\" +\n",
    "                                  f\"{outlierCC:.2f}\", fa.name], loc=\"lower left\", bbox_to_anchor=(-0.60, 0.0))\n",
    "        fig.savefig(nameBase + \".pdf\")\n",
    "        print(f\"Saved {os.path.basename(nameBase)}.\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    plotCosineFit()\n",
    "\n",
    "\n",
    "def crossCorrelate(fa):\n",
    "    dfPairss = pd.read_csv(os.path.join(fa.dir, \"catalog-pair.csv\"))\n",
    "    futures = []\n",
    "\n",
    "    ccFiles = [os.path.join(fa.ccDir, x) for x in os.listdir(fa.ccDir)]\n",
    "    for f in ccFiles:\n",
    "        os.remove(f)\n",
    "    # for r in dfPairss.itertuples(index=True):\n",
    "    #     print(f\"{r.Index + 1}/{dfPairss.shape[0]} ...\")\n",
    "    #     cc(fa, r.id1, r.t1, r.lat1, r.lon1, r.mag1, r.id2, r.t2, r.lat2, r.lon2, r.mag2)\n",
    "    with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "        for r in dfPairss.itertuples(index=True):\n",
    "            futures.append(executor.submit(cc, fa, r.id1, r.t1, r.lat1,\n",
    "                                           r.lon1, r.mag1, r.id2, r.t2, r.lat2, r.lon2, r.mag2))\n",
    "        [future.result() for future in as_completed(futures)]\n",
    "\n",
    "\n",
    "class RelocationProcedure(AbstractFaultProcess):\n",
    "\n",
    "    def __init__(self, name, *args, **kwargs):\n",
    "        super().__init__(name)\n",
    "        self.waveDir = os.path.join(self.dir, \"waves\")\n",
    "        if not os.path.isdir(self.waveDir):\n",
    "            os.mkdir(self.waveDir)\n",
    "        self.ccDir = os.path.join(self.dir, \"cc\")\n",
    "        if not os.path.isdir(self.ccDir):\n",
    "            os.mkdir(self.ccDir)\n",
    "        with open(os.path.join(self.dir, \"mt.json\")) as fp:\n",
    "            self.fm = json.load(fp)\n",
    "        self.relocationConfig = kwargs\n",
    "\n",
    "\n",
    "def optimize(fa):\n",
    "\n",
    "    def formEdge(x: dict, linkNum: int = 8):\n",
    "        return \\\n",
    "            (not np.isnan(x['B'])) and \\\n",
    "            (not np.isinf(x['dB'])) and \\\n",
    "            x['numHighCC'] >= linkNum\n",
    "\n",
    "    def buildPairGraph():\n",
    "        dfPairs = pd.read_csv(os.path.join(fa.dir, \"catalog-pair.csv\"))\n",
    "        files = (x for x in os.listdir(fa.ccDir) if x.endswith(\".json\"))\n",
    "        pairkey2row = {(r.id1, r.id2): r for r in dfPairs.itertuples()}\n",
    "\n",
    "        G = nx.Graph()\n",
    "        for file in files:\n",
    "            with open(os.path.join(fa.ccDir, file), \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                if formEdge(data):\n",
    "                    info = os.path.splitext(file)[0].split('-')\n",
    "                    id1, id2 = map(str, info)\n",
    "                    r = pairkey2row.get((id1, id2), None)\n",
    "                    if not r:\n",
    "                        continue\n",
    "                    if id1 != r.id1 or id2 != r.id2:\n",
    "                        raise ValueError(\"pair id mismatch.\")\n",
    "\n",
    "                    # duplicate hashing is ignored\n",
    "                    G.add_node(id1,\n",
    "                               lat=r.lat1, lon=r.lon1, mag=r.mag1,\n",
    "                               t=r.t1, group=-1)\n",
    "                    G.add_node(id2,\n",
    "                               lat=r.lat2, lon=r.lon2, mag=r.mag2,\n",
    "                               t=r.t2, group=-1)\n",
    "                    G.add_edge(id1, id2,\n",
    "                               B=data['B'], C=data['C'],\n",
    "                               dB=data['dB'], dC=data['dC'])\n",
    "        return G\n",
    "\n",
    "    def traverseGraph(G, relocateOneWay=True):\n",
    "        # BFS, relocate one group of events by single reference abiding shortest path\n",
    "        # This, however, does not account for discrepancy with direct observation and\n",
    "        # new locations. Using optimization is perferred, see below.\n",
    "\n",
    "        # If not relocate, it aims to idenfity groups and label accordingly.\n",
    "        def traverse(n):\n",
    "            queueSet.add(n)\n",
    "            for k in G.adj[n].keys():\n",
    "                if (not visited[k]) and (k not in queueSet):\n",
    "                    if relocateOneWay:\n",
    "                        B, C = G.adj[n][k]['B'], G.adj[n][k]['C']\n",
    "                        # all pairs id1 is more recent than id2\n",
    "                        C = C if IDTimePair[n] > IDTimePair[k] else C + 180.0  # only change `B` or `C`, not both\n",
    "                        lat1, lon1 = G.nodes.data(\n",
    "                            'lat')[n], G.nodes.data('lon')[n]\n",
    "                        lon2, lat2, _ = geod.fwd(lon1, lat1, C, B*1e3)\n",
    "                        G.nodes[k].update(lat=lat2, lon=lon2)\n",
    "                    visited[k] = True\n",
    "                    IDTimePairDynamic.pop(k, None)\n",
    "                    G.nodes[k].update(group=groupid)\n",
    "                    Q.append(k)\n",
    "                    queueSet.add(k)\n",
    "\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        Q = deque()\n",
    "        queueSet = set()\n",
    "        visited = dict(G.nodes.data('visited'))\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "        IDTimePairDynamic = dict(G.nodes.data(\"t\"))\n",
    "        groupid = 1\n",
    "\n",
    "        while count := sum([1 for (k, v) in visited.items() if not v]) > 0:\n",
    "            # use newest event as reference\n",
    "            startID = max(IDTimePairDynamic, key=IDTimePairDynamic.get)\n",
    "            Q.append(startID)\n",
    "            visited[startID] = True\n",
    "            IDTimePairDynamic.pop(startID)\n",
    "            G.nodes[startID].update(group=groupid)\n",
    "            while Q:\n",
    "                traverse(Q.popleft())\n",
    "            groupid += 1\n",
    "        return G\n",
    "\n",
    "    def traverseOptimizeGraph(G):\n",
    "        Q = deque()\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        optimized = dict(G.nodes.data('visited'))\n",
    "        queueSet = set()\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "        IDTimePair2 = dict(G.nodes.data(\"t\"))\n",
    "\n",
    "        def optimizeNodeLocation(node):\n",
    "\n",
    "            def objective(x, lons, lats, B, C, dB, dC):\n",
    "                res = 0.0\n",
    "                for i in range(len(lons)):\n",
    "                    fz, _, dist = geod.inv(x[0], x[1], lons[i], lats[i])\n",
    "                    fz, dist = _normalize_fz_dist(fz, dist)\n",
    "                    res += np.abs(angularDiff(fz, C[i]) / dC[i]) + \\\n",
    "                        np.abs((dist/1e3 - B[i]) / dB[i])\n",
    "                return res\n",
    "\n",
    "            ks = G.adj[node].keys()\n",
    "            queueSet.add(node)\n",
    "            IDTimePair.pop(node, None)\n",
    "            for k in ks:\n",
    "                if (not optimized[k]) and (k not in queueSet):\n",
    "                    Q.append(k)\n",
    "                    queueSet.add(k)\n",
    "                    IDTimePair.pop(k, None)\n",
    "\n",
    "            lons = np.array([G.nodes.data('lon')[k] for k in ks])\n",
    "            lats = np.array([G.nodes.data('lat')[k] for k in ks])\n",
    "            x0 = np.array([angularMean(lons), angularMean(lats)])\n",
    "            Bs = np.array([G.adj[node][k]['B'] for k in ks])\n",
    "            Cs = np.array([G.adj[node][k]['C'] for k in ks])\n",
    "            dBs = np.array([G.adj[node][k]['dB'] for k in ks])\n",
    "            dCs = np.array([G.adj[node][k]['dC'] for k in ks])\n",
    "            for i, k in enumerate(ks):\n",
    "                # all pairs id1 is more recent than id2\n",
    "                if IDTimePair2[node] < IDTimePair2[k]:  # correction for direction\n",
    "                    Cs[i] += 180.0\n",
    "                if Bs[i] < 0:  # correction for negative distance\n",
    "                    Bs[i] *= -1\n",
    "                    Cs[i] += 180.0\n",
    "                Cs[i] %= 360  # correction for azimuth angle between [0, 360)\n",
    "            res = minimize(objective, x0, args=(\n",
    "                lons, lats, Bs, Cs, dBs, dCs), method=\"Nelder-Mead\")\n",
    "            G.nodes[node].update(lat=res.x[1], lon=res.x[0])\n",
    "            optimized[node] = True\n",
    "\n",
    "        while count := sum([1 for (k, v) in optimized.items() if not v]) > 0:\n",
    "            # startID = min([k for (k, v) in optimized.items() if not v])\n",
    "            startID = max(IDTimePair, key=IDTimePair.get)\n",
    "            Q.append(startID)\n",
    "            IDTimePair.pop(startID)\n",
    "            while Q:\n",
    "                optimizeNodeLocation(Q.popleft())\n",
    "        return G\n",
    "\n",
    "    def optimizeLocation(G, relocateTwoWay=True, relocateTwoWayIter=5, relocateGlobal=True):\n",
    "        if relocateTwoWay:\n",
    "            for _ in range(relocateTwoWayIter):\n",
    "                traverseOptimizeGraph(G)\n",
    "\n",
    "        data = {k: G.nodes[k] for k in G.nodes}\n",
    "        uniqueGroupID = set([data[k]['group'] for k in data.keys()])\n",
    "        linkContent = {x: [] for x in [\n",
    "            \"id1\", \"t1\", \"lat1\", \"lon1\", \"mag1\",\n",
    "            \"id2\", \"t2\", \"lat2\", \"lon2\", \"mag2\",\n",
    "            \"dist\", \"B\", \"dB\",\n",
    "            \"azi\", \"C\", \"dC\",\n",
    "            \"group\", \"weight\",\n",
    "        ]}\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "\n",
    "        for gid in uniqueGroupID:\n",
    "            es = [x for x in G.edges if data[x[0]]['group'] == gid]\n",
    "            es.sort(key=lambda x: (IDTimePair[x[0]], IDTimePair[x[1]]), reverse=True)\n",
    "            pks = list(set(itertools.chain(*es)))\n",
    "            pks.sort(key=lambda x: IDTimePair[x], reverse=True)\n",
    "            id2t = {k: IDTimePair[k] for k in pks}\n",
    "            masterId = max(id2t, key=id2t.get)\n",
    "            masterLat, masterLon = data[masterId]['lat'], data[masterId]['lon']\n",
    "            key2id = {pks[i]: i for i in range(len(pks))}\n",
    "            Bs = [G.edges[x]['B'] for x in es]\n",
    "            Cs = [G.edges[x]['C'] for x in es]\n",
    "            dBs = [G.edges[x]['dB'] for x in es]\n",
    "            dCs = [G.edges[x]['dC'] for x in es]\n",
    "            lat0 = np.array([data[x]['lat'] for x in pks])\n",
    "            lon0 = np.array([data[x]['lon'] for x in pks])\n",
    "            u0 = [[lat0[i], lon0[i]] for i in range(len(pks))]\n",
    "            u0 = list(itertools.chain(*u0))\n",
    "            u0.append(1)  # initial weight\n",
    "\n",
    "            if not relocateGlobal:\n",
    "                x = u0\n",
    "            else:\n",
    "                avglat0, avglon0 = angularMean(lat0), angularMean(lon0)\n",
    "                searchRange = 10.0  # within 10 degree, should be sufficient\n",
    "                # not rigorously right,\n",
    "                # but we don't have events at polar region or date changing line\n",
    "                lblat = np.clip(lat0 - searchRange, -90, 90)\n",
    "                ublat = np.clip(lat0 + searchRange, -90, 90)\n",
    "                lblon = np.clip(lon0 - searchRange, -180, 180)\n",
    "                ublon = np.clip(lon0 + searchRange, -180, 180)\n",
    "\n",
    "                lb = [[x, y] for x, y in zip(lblat, lblon)]\n",
    "                lb = list(itertools.chain(*lb))\n",
    "                lb.append(0.2)  # lower bound for relative weight\n",
    "                ub = [[x, y] for x, y in zip(ublat, ublon)]\n",
    "                ub = list(itertools.chain(*ub))\n",
    "                ub.append(2)  # upper bound for relative weight\n",
    "\n",
    "                objective, masterLatitudeConstraint, masterLongitudeConstraint = \\\n",
    "                    objectiveFuncFactory(\n",
    "                        masterId, masterLat, masterLon, Bs, Cs, dBs, dCs, key2id, es)\n",
    "\n",
    "                opt = nlopt.opt(nlopt.LN_COBYLA, 2*len(pks)+1)\n",
    "                opt.set_min_objective(objective)\n",
    "                opt.add_equality_constraint(masterLatitudeConstraint, 1e-6)\n",
    "                opt.add_equality_constraint(masterLongitudeConstraint, 1e-6)\n",
    "                opt.set_lower_bounds(lb)\n",
    "                opt.set_upper_bounds(ub)\n",
    "                opt.set_xtol_rel(1e-6)\n",
    "                x = opt.optimize(u0)\n",
    "\n",
    "            for e in es:\n",
    "                _add_link_content(linkContent, G, e, x, key2id, geod, IDTimePair)\n",
    "\n",
    "        pd.DataFrame(linkContent).to_csv(os.path.join(\n",
    "            fa.dir, \"catalog-link.csv\"), index=False)\n",
    "        content = {'id': [], 'time': [], 'lat': [],\n",
    "                   'lon': [], 'mag': [], 'group': []}\n",
    "        content['id'].extend(linkContent['id1'])\n",
    "        content['id'].extend(linkContent['id2'])\n",
    "        content['time'].extend(linkContent['t1'])\n",
    "        content['time'].extend(linkContent['t2'])\n",
    "        content['lat'].extend(linkContent['lat1'])\n",
    "        content['lat'].extend(linkContent['lat2'])\n",
    "        content['lon'].extend(linkContent['lon1'])\n",
    "        content['lon'].extend(linkContent['lon2'])\n",
    "        content['mag'].extend(linkContent['mag1'])\n",
    "        content['mag'].extend(linkContent['mag2'])\n",
    "        content['group'].extend(linkContent['group'])\n",
    "        content['group'].extend(linkContent['group'])\n",
    "        pd.DataFrame(content).drop_duplicates(subset=['id'], keep='last').sort_values(\n",
    "            by=\"time\").to_csv(os.path.join(fa.dir, \"catalog-relocated.csv\"), index=False)\n",
    "\n",
    "    def _add_link_content(linkContent, G, e, sol, key2id, geod, IDTimePair):\n",
    "        # repeated code\n",
    "        # all pairs id1 is more recent than id2\n",
    "        if IDTimePair[e[0]] < IDTimePair[e[1]]:\n",
    "            e = (e[1], e[0])\n",
    "        id1, id2 = key2id[e[0]], key2id[e[1]]\n",
    "\n",
    "        lat1, lon1 = sol[2*id1], sol[2*id1+1]\n",
    "        lat2, lon2 = sol[2*id2], sol[2*id2+1]\n",
    "\n",
    "        linkContent['id1'].append(e[0])\n",
    "        linkContent['t1'].append(G.nodes[e[0]]['t'])\n",
    "        linkContent['lat1'].append(lat1)\n",
    "        linkContent['lon1'].append(lon1)\n",
    "        linkContent['mag1'].append(G.nodes[e[0]]['mag'])\n",
    "        linkContent['id2'].append(e[1])\n",
    "        linkContent['t2'].append(G.nodes[e[1]]['t'])\n",
    "        linkContent['lat2'].append(lat2)\n",
    "        linkContent['lon2'].append(lon2)\n",
    "        linkContent['mag2'].append(G.nodes[e[1]]['mag'])\n",
    "        fz, dist = _objection_val(sol, e, key2id, geod)\n",
    "        linkContent['dist'].append(dist/1e3)\n",
    "        linkContent['azi'].append(fz)\n",
    "        bb, cc = G.edges[e]['B'], G.edges[e]['C'] % 360\n",
    "        cc, bb = _normalize_fz_dist(cc, bb)\n",
    "        linkContent['B'].append(bb)\n",
    "        linkContent['C'].append(cc)\n",
    "        linkContent['dB'].append(G.edges[e]['dB'])\n",
    "        linkContent['dC'].append(G.edges[e]['dC'] % 360)\n",
    "        linkContent['group'].append(G.nodes[e[0]]['group'])\n",
    "        linkContent['weight'].append(sol[-1])\n",
    "\n",
    "    def mergeCatalogue():\n",
    "        content_strs = ['id', 'time', 'lat', 'lon', 'mag', 'group']\n",
    "        content = {x: [] for x in content_strs}\n",
    "        dforg = pd.read_csv(os.path.join(fa.dir, \"catalog.csv\"))\n",
    "        dfcc = pd.read_csv(os.path.join(fa.dir, \"catalog-relocated.csv\"))\n",
    "        relocatedID = set(dfcc[\"id\"].to_list())\n",
    "        for r in dfcc.itertuples():\n",
    "            content['id'].append(r.id)\n",
    "            content['time'].append(r.time)\n",
    "            content['lat'].append(r.lat)\n",
    "            content['lon'].append(r.lon)\n",
    "            content['mag'].append(r.mag)\n",
    "            content['group'].append(r.group)\n",
    "        for r in dforg.itertuples():\n",
    "            if r.id not in relocatedID:\n",
    "                mw = mag2mw(r.mag, r.magType)\n",
    "                content['id'].append(r.id)\n",
    "                content['time'].append(r.time)\n",
    "                content['lat'].append(r.lat)\n",
    "                content['lon'].append(r.lon)\n",
    "                content['mag'].append(mw)\n",
    "                content['group'].append(-1)\n",
    "        pd.DataFrame(content).sort_values(by=[\"time\"]).to_csv(\n",
    "            os.path.join(fa.dir, \"catalog-merged.csv\"), index=False)\n",
    "\n",
    "    def _objection_val(sol, e: list, key2id: dict, geod):\n",
    "        id1, id2 = key2id[e[0]], key2id[e[1]]\n",
    "        lat1, lon1 = sol[2*id1], sol[2*id1+1]\n",
    "        lat2, lon2 = sol[2*id2], sol[2*id2+1]\n",
    "        fz, _, dist = geod.inv(lon1, lat1, lon2, lat2)\n",
    "        fz, dist = _normalize_fz_dist(fz, dist)\n",
    "        return fz, dist\n",
    "\n",
    "    def _normalize_fz_dist(fz: float, dist: float):\n",
    "        if dist < 0:\n",
    "            dist *= -1\n",
    "            fz += 180.0\n",
    "        fz %= 360\n",
    "        return fz, dist\n",
    "\n",
    "    def objectiveFuncFactory(masterID, masterLat, masterLon, Bs, Cs, dBs, dCs, key2id, es, weight=1.0):\n",
    "        # weight denotes relative contribution between `dist` and `azi`\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        _B = copy.copy(Bs)\n",
    "        _C = list(map(lambda x: x % 360, Cs))\n",
    "        _dB = copy.copy(dBs)\n",
    "        _dC = list(map(lambda x: x % 360, dCs))\n",
    "        for i in range(len(_B)):\n",
    "            _C[i], _B[i] = _normalize_fz_dist(_C[i], _B[i])\n",
    "            _dC[i], _dB[i] = _normalize_fz_dist(_dC[i], _dB[i])\n",
    "            if _dC[i] > 180:\n",
    "                _dC[i] = 360.0 - _dC[i]\n",
    "\n",
    "        _dB2 = [x**2 for x in _dB]\n",
    "        _dC2 = [x**2 for x in _dC]\n",
    "\n",
    "        def objective(x, grad):\n",
    "            # x = [lat_1, lon_1, lat_2, lon_2, ..., lat_n, lon_n, weight]\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"Use gradient free method!\")\n",
    "\n",
    "            y = 0.0\n",
    "            for i, e in enumerate(es):\n",
    "                fz, dist = _objection_val(x, e, key2id, geod)\n",
    "                y += np.abs((dist/1e3 - _B[i]) / _dB[i]) + \\\n",
    "                    x[-1] * np.abs(angularDiff(fz, _C[i]) / _dC[i])\n",
    "            return np.sqrt(y)\n",
    "\n",
    "        def masterLatitudeConstraint(x, grad):\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"Use gradient free method!\")\n",
    "            return x[2 * key2id[masterID]] - masterLat\n",
    "\n",
    "        def masterLongitudeConstraint(x, grad):\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"User gradient free method!\")\n",
    "            return x[2 * key2id[masterID] + 1] - masterLon\n",
    "\n",
    "        return objective, masterLatitudeConstraint, masterLongitudeConstraint\n",
    "\n",
    "    G = buildPairGraph()\n",
    "    G = traverseGraph(G, True)\n",
    "    optimizeLocation(G, relocateTwoWay=True, relocateGlobal=False)\n",
    "    mergeCatalogue()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    names = dfFaults[\"Name\"].to_list()\n",
    "    for name in names:\n",
    "        print(name)\n",
    "        f = RelocationProcedure(name.strip())\n",
    "        crossCorrelate(f)\n",
    "        optimize(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0995aa0",
   "metadata": {},
   "source": [
    "# Relocate the single pair of earthquake events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9f768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import operator\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque, namedtuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nlopt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from obspy import read\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy.io.sac.util import SacIOError\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "from pyproj import Geod\n",
    "from scipy.optimize import curve_fit, least_squares, minimize\n",
    "\n",
    "from GetData import AbstractFaultProcess\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def cc(fa, id1, t1, lat1, lon1, mag1, id2, t2, lat2, lon2, mag2):\n",
    "    # cannot serialize pandas frame, which sliently quits multiprocessor\n",
    "\n",
    "    outlierCC = fa.relocationConfig.get(\"outlierCC\", 0.70)\n",
    "    outlierDt = fa.relocationConfig.get(\"outlierDt\", 35)\n",
    "    numCCToFit = fa.relocationConfig.get(\"numCCToFit\", 8)\n",
    "    vgroup = fa.relocationConfig.get(\"vgroup\", 3.75)\n",
    "    outlierDx = vgroup * outlierDt\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "    def badCoverage(azi, maxgap=120.0):  # exclude those with azimuth gap larger than `maxgap`\n",
    "        if len(azi) == 0:\n",
    "            return False\n",
    "        _azi = sorted([x % 360 for x in azi])\n",
    "        diff = [_azi[i+1] - _azi[i] for i in range(len(_azi) - 1)]\n",
    "        diff.append(360 - _azi[-1] + _azi[0])\n",
    "        return np.any(np.array(diff) > maxgap)\n",
    "\n",
    "    def residual_cosine(x, azi, dt):\n",
    "        return x[0] + x[1] * np.cos(np.deg2rad(azi - x[2])) - dt\n",
    "\n",
    "    def fit_cosine_ls_norm(azi, dx, cc, u0):\n",
    "        mask = np.logical_and(cc >= outlierCC, np.abs(dx) <= outlierDx)\n",
    "        _azi = azi[mask]\n",
    "        _dx = dx[mask]\n",
    "        if len(_azi) < numCCToFit or badCoverage(_azi):\n",
    "            return np.array([np.nan, np.nan, np.nan]), np.array([np.inf, np.inf, np.inf])\n",
    "\n",
    "        # initial solution affect the local minimum obtained\n",
    "        res = least_squares(residual_cosine, [0., u0, 0.], args=(\n",
    "            _azi, _dx), loss='soft_l1', jac='3-point')\n",
    "\n",
    "        # On way of uncertainty, but inappropriate since we use other forms of norm\n",
    "        # https://stackoverflow.com/a/21844726/8697614\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # pcov = cov * (res.fun ** 2).sum() / (len(_azi) - len(res.x)) # reduced chi-square distribution\n",
    "        # err = np.sqrt(np.diagonal(pcov))\n",
    "\n",
    "        # Another way of uncertainty, still assume your function value is chi-square\n",
    "        # https://stackoverflow.com/a/53489234/8697614, notice `dx[2] can exceed 360`\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # err2 = np.sqrt(np.diagonal(cov) * np.abs(res.cost))\n",
    "        # return res.x, err\n",
    "\n",
    "        # The third way: bootstrap\n",
    "        # This is what McGuire did, assuming 1s error in dt\n",
    "        # err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=1.0 * vgroup)\n",
    "        # we instead use fitting residual as the error for bootstrap\n",
    "        err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=np.std(\n",
    "            residual_cosine(res.x, _azi, _dx)))\n",
    "        return res.x, err\n",
    "\n",
    "    def bootstrap_uncertainty(x, y, p0, numiters=400, stderr=3.75, nsigma=1.0):\n",
    "        x_cmt, x_dist, x_azi = [], [], []\n",
    "        n = len(y)\n",
    "        for i in range(numiters):\n",
    "            yy = y + np.random.default_rng().normal(0.0, stderr, n)\n",
    "            res = least_squares(residual_cosine, p0, args=(\n",
    "                x, yy), loss='soft_l1', jac='3-point')\n",
    "            x_cmt.append(res.x[0])\n",
    "            x_dist.append(res.x[1])\n",
    "            x_azi.append(res.x[2])\n",
    "        return nsigma*np.std(x_cmt), nsigma*np.std(x_dist), nsigma*np.std(x_azi)\n",
    "\n",
    "    fm1, fm2 = fa.fm[str(id1)][\"np1\"], fa.fm[str(id2)][\"np1\"]\n",
    "    wv1, wv2 = [], []\n",
    "    content = {x: [] for x in [\"azi\", \"dt\",\n",
    "                               \"cc\", \"net\", \"sta\", \"staLat\", \"staLon\"]}\n",
    "    nameBase = os.path.join(\n",
    "        fa.ccDir, '-'.join(map(str, [id1, id2])))\n",
    "    dfStations = pd.read_csv(os.path.join(fa.dir, \"stations.csv\"))\n",
    "    for rs in dfStations.itertuples():\n",
    "        waveName1 = str(id1) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        waveName2 = str(id2) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        wavePath1 = os.path.join(fa.waveDir, waveName1)\n",
    "        wavePath2 = os.path.join(fa.waveDir, waveName2)\n",
    "\n",
    "        if (\n",
    "            np.isnan(rs.dist) or\n",
    "            not os.path.isfile(wavePath1) or\n",
    "            not os.path.isfile(wavePath2) or\n",
    "            os.path.getsize(wavePath1) == 0 or\n",
    "            os.path.getsize(wavePath2) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        midLon, midLat = angularMean(\n",
    "            [lon1, lon2]), angularMean([lat1, lat2])\n",
    "        fz, _, _ = geod.inv(midLon, midLat, rs.lon,\n",
    "                            rs.lat)  # fz: (-180, 180)\n",
    "        try:\n",
    "            st1 = read(wavePath1, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath1)\n",
    "            continue\n",
    "        try:\n",
    "            st2 = read(wavePath2, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath2)\n",
    "            continue\n",
    "\n",
    "        if st1[0].stats[\"sampling_rate\"] != st2[0].stats[\"sampling_rate\"]:\n",
    "            maxSamplingRate = max(st1[0].stats[\"sampling_rate\"], st2[0].stats[\"sampling_rate\"])\n",
    "            for ss in [st1, st2]:\n",
    "                if maxSamplingRate != ss[0].stats[\"sampling_rate\"]:\n",
    "                    ss.resample(maxSamplingRate, no_filter=True)\n",
    "\n",
    "        cc = correlate(\n",
    "            st1[0], st2[0], demean=True, normalize='naive',\n",
    "            shift=min(st1[0].stats.npts, st2[0].stats.npts, int(\n",
    "                np.round(outlierDt / st1[0].stats.delta * 2.0)))\n",
    "        )\n",
    "        # consider only positive cc\n",
    "        shift, value = xcorr_max(cc, abs_max=False)\n",
    "        dt = st1[0].stats.delta * shift\n",
    "        content['azi'].append(fz)\n",
    "        content['dt'].append(dt)\n",
    "        content['cc'].append(value)\n",
    "        content['net'].append(rs.net)\n",
    "        content['sta'].append(rs.sta)\n",
    "        content['staLat'].append(rs.lat)\n",
    "        content['staLon'].append(rs.lon)\n",
    "        wv1.append(st1[0].data)\n",
    "        wv2.append(st2[0].data)\n",
    "\n",
    "    pd.DataFrame(content).to_csv(nameBase + \".csv\", index=False)\n",
    "    if len(content[\"dt\"]) < numCCToFit:\n",
    "        return\n",
    "\n",
    "    yy = np.array(content['dt']) * vgroup\n",
    "    xx = np.array(content['azi'])\n",
    "    # initial guess of distance, important for least square fit\n",
    "    _, _, u0 = geod.inv(lon1, lat1, lon2, lat2)\n",
    "    popt, pcov = fit_cosine_ls_norm(\n",
    "        xx, yy, np.array(content['cc']), np.abs(u0 / 1e3))\n",
    "    A, B, C = popt\n",
    "    dA, dB, dC = pcov\n",
    "    highcc = list(filter(lambda x: np.abs(x) >= outlierCC, content['cc']))\n",
    "    summary = {\n",
    "        'A': popt[0], 'B': popt[1], 'C': popt[2],\n",
    "        'dA': pcov[0], 'dB': pcov[1], 'dC': pcov[2],\n",
    "        'numHighCC': len(highcc),\n",
    "    }\n",
    "    with open(nameBase + \".json\", \"w\") as fp:\n",
    "        json.dump(summary, fp, indent=4)\n",
    "\n",
    "    def plotCosineFit():\n",
    "        fig = plt.figure(figsize=(8, 10))\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[2, 1])\n",
    "        ax0 = fig.add_subplot(gs[0, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(\n",
    "            gs[1, 0], projection=ccrs.AzimuthalEquidistant(lon1, lat1))\n",
    "        ax2.stock_img()\n",
    "        ax2.scatter(lon1, lat1, s=20, marker=\"*\", c=\"orange\",\n",
    "                    transform=ccrs.PlateCarree(), zorder=2)\n",
    "\n",
    "        ax3 = fig.add_axes([0.55, 0.89, 0.1, 0.1])\n",
    "        ax3.set_aspect(\"equal\")\n",
    "        if np.isnan(fm1[0]):\n",
    "            bc1 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.75), width=0.4)\n",
    "        else:\n",
    "            bc1 = beach(fm1, facecolor=\"tab:blue\", xy=(0.5, 0.75), width=0.4)\n",
    "        if np.isnan(fm2[0]):\n",
    "            bc2 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.25), width=0.4)\n",
    "        else:\n",
    "            bc2 = beach(fm2, facecolor=\"tab:red\", xy=(0.5, 0.25), width=0.4)\n",
    "        ax3.add_collection(bc1)\n",
    "        ax3.add_collection(bc2)\n",
    "        ax3.axis('off')\n",
    "\n",
    "        for i in range(len(wv1)):\n",
    "            # do not set `step=delta` which may cause length mismatch\n",
    "            y1 = np.arange(0.0, len(wv1[i])) * st1[0].stats.delta\n",
    "            y2 = np.arange(0.0, len(wv2[i])) * st2[0].stats.delta\n",
    "            w1 = wv1[i] - np.nanmean(wv1[i])\n",
    "            w2 = wv2[i] - np.nanmean(wv2[i])\n",
    "            w1 = w1 / np.nanmax(np.abs(w1)) * 8\n",
    "            w2 = w2 / np.nanmax(np.abs(w2)) * 8\n",
    "\n",
    "            isoutlier = not (np.abs(content[\"dt\"][i]) <= outlierDt and np.abs(\n",
    "                content[\"cc\"][i]) >= outlierCC)\n",
    "            linestyle = '--' if isoutlier else '-'\n",
    "            alpha = 0.2 if isoutlier else 1.0\n",
    "            fc = 'white' if isoutlier else 'tab:green'\n",
    "            ax0.plot(y1, w1 + content[\"azi\"][i], color=\"tab:blue\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax0.plot(y2, w2 + content[\"azi\"][i], color=\"tab:red\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax1.scatter(content[\"dt\"][i], content[\"azi\"][i],\n",
    "                        fc=fc, s=15, marker=\"o\", ec=\"black\", zorder=2)\n",
    "\n",
    "            trajectorColor = \"gray\" if isoutlier else \"tab:pink\"\n",
    "            trajectorStyle = \"--\" if isoutlier else \"-\"\n",
    "            ax2.plot([lon1, content[\"staLon\"][i]], [lat1, content[\"staLat\"][i]],\n",
    "                     color=trajectorColor, transform=ccrs.Geodetic(), zorder=1, linestyle=trajectorStyle)\n",
    "\n",
    "        xx = np.arange(-180, 180, 1.0)\n",
    "        if np.isnan(A):\n",
    "            yy = np.array([np.nan for _ in xx])\n",
    "            fittingColor = \"gray\"\n",
    "        else:\n",
    "            yy = A + B * np.cos(np.deg2rad(xx - C))\n",
    "            yy /= vgroup\n",
    "            fittingColor = \"tab:orange\"\n",
    "\n",
    "        ax1.plot(yy, xx, c=fittingColor, zorder=1)\n",
    "        _b, _c = B, C\n",
    "        if _b < 0:\n",
    "            _b *= -1\n",
    "            _c += 180\n",
    "        _c %= 360\n",
    "        ax1.text(0.5, 1.05, \"$\\Delta x = \\;$\" + f\"{_b:.0f}\" + \"$\\;\\pm \\;$\" + f\"{dB:.0f}\" + \"$\\;\\mathrm{km}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax1.text(0.5, 1.12, r\"$\\theta = \\;$\" + f\"{_c:.1f}\" + \"$\\;\\pm \\;$\" + f\"{dC:.1f}\" + \"$^{\\circ}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax0.set_ylim([-180, 180])\n",
    "        ax1.set_ylim([-180, 180])\n",
    "        ax1.set_xlim([-outlierDt, outlierDt])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.grid(True, which=\"major\", color='gray',\n",
    "                 linestyle='--', linewidth=1)\n",
    "        ax1.set_xlabel(\"$\\Delta T$ (second)\")\n",
    "        ax0.set_xlabel(\"Time (second)\")\n",
    "        ax0.set_ylabel(\"Azimuth (deg)\")\n",
    "        ax0.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "        l1, = ax0.plot([], [], color=\"tab:blue\")\n",
    "        l2, = ax0.plot([], [], color=\"tab:red\")\n",
    "        _tstr1 = datetime.fromisoformat(t1)\n",
    "        _tstr1 = _tstr1.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag1:.1f}')\n",
    "        _tstr2 = datetime.fromisoformat(t2)\n",
    "        _tstr2 = _tstr2.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag2:.1f}')\n",
    "        ax0.legend([l1, l2], [_tstr1, _tstr2],\n",
    "                   loc=\"upper center\", bbox_to_anchor=(0.5, 1.18))\n",
    "\n",
    "        l1, = ax1.plot([], [], color=\"tab:orange\")\n",
    "        l2 = ax0.scatter([], [], fc='tab:green', s=15, marker=\"o\", ec=\"black\")\n",
    "        l3 = ax0.scatter([], [], fc='white', s=15, marker=\"o\", ec=\"black\")\n",
    "        ax1.legend([l1, l2, l3], [\"Cosine Fitting\", \"Observation (cc $\\geq \\;$\" +\n",
    "                                  f\"{outlierCC:.2f})\", f\"Observation (cc $<$ {outlierCC:.2f})\"], loc=\"lower center\", bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "        l1, = ax2.plot([], [], color=\"tab:pink\", linestyle=\"-\")\n",
    "        l2, = ax2.plot([], [], color=\"gray\", linestyle=\"--\")\n",
    "        l3 = ax2.scatter([], [], s=20, marker=\"*\", c=\"orange\")\n",
    "        ax2.legend([l1, l2, l3], [\"$\\mathrm{cc} \\geq \\;$\"+f\"{outlierCC:.2f}\", \"$\\mathrm{cc} <\\;$\" +\n",
    "                                  f\"{outlierCC:.2f}\", fa.name], loc=\"lower left\", bbox_to_anchor=(-0.60, 0.0))\n",
    "        fig.savefig(nameBase + \".pdf\")\n",
    "        print(f\"Saved {os.path.basename(nameBase)}.\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    plotCosineFit()\n",
    "\n",
    "\n",
    "def crossCorrelate(fa):\n",
    "    dfPairss = pd.read_csv(os.path.join(fa.dir, \"catalog-pair.csv\"))\n",
    "    futures = []\n",
    "\n",
    "    ccFiles = [os.path.join(fa.ccDir, x) for x in os.listdir(fa.ccDir)]\n",
    "    for f in ccFiles:\n",
    "        os.remove(f)\n",
    "    # for r in dfPairss.itertuples(index=True):\n",
    "    #     print(f\"{r.Index + 1}/{dfPairss.shape[0]} ...\")\n",
    "    #     cc(fa, r.id1, r.t1, r.lat1, r.lon1, r.mag1, r.id2, r.t2, r.lat2, r.lon2, r.mag2)\n",
    "    with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "        for r in dfPairss.itertuples(index=True):\n",
    "            futures.append(executor.submit(cc, fa, r.id1, r.t1, r.lat1,\n",
    "                                           r.lon1, r.mag1, r.id2, r.t2, r.lat2, r.lon2, r.mag2))\n",
    "        [future.result() for future in as_completed(futures)]\n",
    "\n",
    "\n",
    "class RelocationProcedure(AbstractFaultProcess):\n",
    "\n",
    "    def __init__(self, name, *args, **kwargs):\n",
    "        super().__init__(name)\n",
    "        self.waveDir = os.path.join(self.dir, \"waves\")\n",
    "        if not os.path.isdir(self.waveDir):\n",
    "            os.mkdir(self.waveDir)\n",
    "        self.ccDir = os.path.join(self.dir, \"cc\")\n",
    "        if not os.path.isdir(self.ccDir):\n",
    "            os.mkdir(self.ccDir)\n",
    "        with open(os.path.join(self.dir, \"mt.json\")) as fp:\n",
    "            self.fm = json.load(fp)\n",
    "        self.relocationConfig = kwargs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7d024",
   "metadata": {},
   "source": [
    "# Find the focal mechanism for the earthquakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd558b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from obspy import read\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy.io.sac.util import SacIOError\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "from pyproj import Geod\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "waveDir = r\"C:\\Users\\student\\Downloads\\OTF2021-1.0.1\\cross-correl-sample\\907\"\n",
    "\n",
    "\n",
    "def cc(id1, t1, lat1, lon1, mag1, id2, t2, lat2, lon2, mag2):\n",
    "    outlierCC = 0.70\n",
    "    outlierDt = 35\n",
    "    numCCToFit = 8\n",
    "    vgroup = 3.75\n",
    "    outlierDx = vgroup * outlierDt\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "    def badCoverage(azi, maxgap=120.0):\n",
    "        if len(azi) == 0:\n",
    "            return False\n",
    "        _azi = sorted([x % 360 for x in azi])\n",
    "        diff = [_azi[i+1] - _azi[i] for i in range(len(_azi) - 1)]\n",
    "        diff.append(360 - _azi[-1] + _azi[0])\n",
    "        return np.any(np.array(diff) > maxgap)\n",
    "\n",
    "    def residual_cosine(x, azi, dt):\n",
    "        return x[0] + x[1] * np.cos(np.deg2rad(azi - x[2])) - dt\n",
    "\n",
    "    def fit_cosine_ls_norm(azi, dx, cc, u0):\n",
    "        mask = np.logical_and(cc >= outlierCC, np.abs(dx) <= outlierDx)\n",
    "        _azi = azi[mask]\n",
    "        _dx = dx[mask]\n",
    "        if len(_azi) < numCCToFit or badCoverage(_azi):\n",
    "            return np.array([np.nan, np.nan, np.nan]), np.array([np.inf, np.inf, np.inf])\n",
    "\n",
    "        res = least_squares(residual_cosine, [0., u0, 0.], args=(\n",
    "            _azi, _dx), loss='soft_l1', jac='3-point')\n",
    "\n",
    "        err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=np.std(\n",
    "            residual_cosine(res.x, _azi, _dx)))\n",
    "        return res.x, err\n",
    "\n",
    "    def bootstrap_uncertainty(x, y, p0, numiters=400, stderr=3.75, nsigma=1.0):\n",
    "        x_cmt, x_dist, x_azi = [], [], []\n",
    "        n = len(y)\n",
    "        for i in range(numiters):\n",
    "            yy = y + np.random.default_rng().normal(0.0, stderr, n)\n",
    "            res = least_squares(residual_cosine, p0, args=(\n",
    "                x, yy), loss='soft_l1', jac='3-point')\n",
    "            x_cmt.append(res.x[0])\n",
    "            x_dist.append(res.x[1])\n",
    "            x_azi.append(res.x[2])\n",
    "        return nsigma * np.std(x_cmt), nsigma * np.std(x_dist), nsigma * np.std(x_azi)\n",
    "\n",
    "    wv1, wv2 = [], []\n",
    "    content = {x: [] for x in [\"azi\", \"dt\", \"cc\", \"net\", \"sta\", \"staLat\", \"staLon\"]}\n",
    "    nameBase = os.path.join(waveDir, \"cc_results\")\n",
    "    dfStations = pd.read_csv(os.path.join(waveDir, \"stations.csv\"))\n",
    "\n",
    "    for rs in dfStations.itertuples():\n",
    "        waveName1 = str(id1) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        waveName2 = str(id2) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        wavePath1 = os.path.join(waveDir, waveName1)\n",
    "        wavePath2 = os.path.join(waveDir, waveName2)\n",
    "\n",
    "        if (\n",
    "            np.isnan(rs.dist) or\n",
    "            not os.path.isfile(wavePath1) or\n",
    "            not os.path.isfile(wavePath2) or\n",
    "            os.path.getsize(wavePath1) == 0 or\n",
    "            os.path.getsize(wavePath2) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        midLon, midLat = angularMean([lon1, lon2]), angularMean([lat1, lat2])\n",
    "        fz, _, _ = geod.inv(midLon, midLat, rs.lon, rs.lat)\n",
    "        try:\n",
    "            st1 = read(wavePath1, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath1)\n",
    "            continue\n",
    "        try:\n",
    "            st2 = read(wavePath2, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath2)\n",
    "            continue\n",
    "\n",
    "        if st1[0].stats[\"sampling_rate\"] != st2[0].stats[\"sampling_rate\"]:\n",
    "            maxSamplingRate = max(st1[0].stats[\"sampling_rate\"], st2[0].stats[\"sampling_rate\"])\n",
    "            for ss in [st1, st2]:\n",
    "                if maxSamplingRate != ss[0].stats[\"sampling_rate\"]:\n",
    "                    ss.resample(maxSamplingRate, no_filter=True)\n",
    "\n",
    "        cc = correlate(\n",
    "            st1[0], st2[0], demean=True, normalize='naive',\n",
    "            shift=min(st1[0].stats.npts, st2[0].stats.npts, int(np.round(outlierDt / st1[0].stats.delta * 2.0)))\n",
    "        )\n",
    "        shift, value = xcorr_max(cc, abs_max=False)\n",
    "        dt = st1[0].stats.delta * shift\n",
    "        content['azi'].append(fz)\n",
    "        content['dt'].append(dt)\n",
    "        content['cc'].append(value)\n",
    "        content['net'].append(rs.net)\n",
    "        content['sta'].append(rs.sta)\n",
    "        content['staLat'].append(rs.lat)\n",
    "        content['staLon'].append(rs.lon)\n",
    "        wv1.append(st1[0].data)\n",
    "        wv2.append(st2[0].data)\n",
    "\n",
    "    pd.DataFrame(content).to_csv(nameBase + \".csv\", index=False)\n",
    "    if len(content[\"dt\"]) < numCCToFit:\n",
    "        return\n",
    "\n",
    "    yy = np.array(content['dt']) * vgroup\n",
    "    xx = np.array(content['azi'])\n",
    "    _, _, u0 = geod.inv(lon1, lat1, lon2, lat2)\n",
    "    popt, pcov = fit_cosine_ls_norm(xx, yy, np.array(content['cc']), np.abs(u0 / 1e3))\n",
    "    A, B, C = popt\n",
    "    dA, dB, dC = pcov\n",
    "    highcc = list(filter(lambda x: np.abs(x) >= outlierCC, content['cc']))\n",
    "    summary = {\n",
    "        'A': popt[0], 'B': popt[1], 'C': popt[2],\n",
    "        'dA': pcov[0], 'dB': pcov[1], 'dC': pcov[2],\n",
    "        'numHighCC': len(highcc),\n",
    "    }\n",
    "    with open(nameBase + \".json\", \"w\") as fp:\n",
    "        json.dump(summary, fp, indent=4)\n",
    "\n",
    "    def plotCosineFit():\n",
    "        fig = plt.figure(figsize=(8, 10))\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[2, 1])\n",
    "        ax0 = fig.add_subplot(gs[0, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(gs[1, 0], projection=ccrs.AzimuthalEquidistant(lon1, lat1))\n",
    "        ax2.stock_img()\n",
    "        ax2.scatter(lon1, lat1, s=20, marker=\"*\", c=\"orange\", transform=ccrs.PlateCarree(), zorder=2)\n",
    "\n",
    "        ax3 = fig.add_axes([0.55, 0.89, 0.1, 0.1])\n",
    "        ax3.set_aspect(\"equal\")\n",
    "        bc1 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.75), width=0.4)\n",
    "       \n",
    "        bc2 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.25), width=0.4)\n",
    "        ax3.add_collection(bc1)\n",
    "        ax3.add_collection(bc2)\n",
    "        ax3.axis('off')\n",
    "\n",
    "        for i in range(len(wv1)):\n",
    "            y1 = np.arange(0.0, len(wv1[i])) * st1[0].stats.delta\n",
    "            y2 = np.arange(0.0, len(wv2[i])) * st2[0].stats.delta\n",
    "            w1 = wv1[i] - np.nanmean(wv1[i])\n",
    "            w2 = wv2[i] - np.nanmean(wv2[i])\n",
    "            w1 = w1 / np.nanmax(np.abs(w1)) * 8\n",
    "            w2 = w2 / np.nanmax(np.abs(w2)) * 8\n",
    "\n",
    "            isoutlier = not (np.abs(content[\"dt\"][i]) <= outlierDt and np.abs(content[\"cc\"][i]) >= outlierCC)\n",
    "            linestyle = '--' if isoutlier else '-'\n",
    "            alpha = 0.2 if isoutlier else 1.0\n",
    "            fc = 'white' if isoutlier else 'tab:green'\n",
    "            ax0.plot(y1, w1 + content[\"azi\"][i], color=\"tab:blue\", linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax0.plot(y2, w2 + content[\"azi\"][i], color=\"tab:red\", linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax1.scatter(content[\"dt\"][i], content[\"azi\"][i], fc=fc, s=15, marker=\"o\", ec=\"black\", zorder=2)\n",
    "\n",
    "            trajectorColor = \"gray\" if isoutlier else \"tab:pink\"\n",
    "            trajectorStyle = \"--\" if isoutlier else \"-\"\n",
    "            ax2.plot([lon1, content[\"staLon\"][i]], [lat1, content[\"staLat\"][i]],\n",
    "                     color=trajectorColor, transform=ccrs.Geodetic(), zorder=1, linestyle=trajectorStyle)\n",
    "\n",
    "        xx = np.arange(-180, 180, 1.0)\n",
    "        if np.isnan(A):\n",
    "            yy = np.array([np.nan for _ in xx])\n",
    "            fittingColor = \"gray\"\n",
    "        else:\n",
    "            yy = A + B * np.cos(np.deg2rad(xx - C))\n",
    "            yy /= vgroup\n",
    "            fittingColor = \"tab:orange\"\n",
    "\n",
    "        ax1.plot(yy, xx, c=fittingColor, zorder=1)\n",
    "        _b, _c = B, C\n",
    "        if _b < 0:\n",
    "            _b *= -1\n",
    "            _c += 180\n",
    "        _c %= 360\n",
    "        ax1.text(0.5, 1.05, \"$\\Delta x = \\;$\" + f\"{_b:.0f}\" + \"$\\;\\pm \\;$\" + f\"{dB:.0f}\" + \"$\\;\\mathrm{km}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax1.text(0.5, 1.12, r\"$\\theta = \\;$\" + f\"{_c:.1f}\" + \"$\\;\\pm \\;$\" + f\"{dC:.1f}\" + \"$^{\\circ}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax0.set_ylim([-180, 180])\n",
    "        ax1.set_ylim([-180, 180])\n",
    "        ax1.set_xlim([-outlierDt, outlierDt])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.grid(True, which=\"major\", color='gray', linestyle='--', linewidth=1)\n",
    "        ax1.set_xlabel(\"$\\Delta T$ (second)\")\n",
    "        ax0.set_xlabel(\"Time (second)\")\n",
    "        ax0.set_ylabel(\"Azimuth (deg)\")\n",
    "        ax0.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "        l1, = ax0.plot([], [], color=\"tab:blue\")\n",
    "        l2, = ax0.plot([], [], color=\"tab:red\")\n",
    "        _tstr1 = datetime.fromisoformat(t1)\n",
    "        _tstr1 = _tstr1.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag1:.1f}')\n",
    "        _tstr2 = datetime.fromisoformat(t2)\n",
    "        _tstr2 = _tstr2.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag2:.1f}')\n",
    "        ax0.legend([l1, l2], [_tstr1, _tstr2], loc=\"upper center\", bbox_to_anchor=(0.5, 1.18))\n",
    "\n",
    "        l1, = ax1.plot([], [], color=\"tab:orange\")\n",
    "        l2 = ax0.scatter([], [], fc='tab:green', s=15, marker=\"o\", ec=\"black\")\n",
    "        l3 = ax0.scatter([], [], fc='white', s=15, marker=\"o\", ec=\"black\")\n",
    "        ax1.legend([l1, l2, l3], [\"Cosine Fitting\", \"Observation (cc $\\geq \\;$\" +\n",
    "                                  f\"{outlierCC:.2f})\", f\"Observation (cc $<$ {outlierCC:.2f})\"], loc=\"lower center\", bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "        l1, = ax2.plot([], [], color=\"tab:pink\", linestyle=\"-\")\n",
    "        l2, = ax2.plot([], [], color=\"gray\", linestyle=\"--\")\n",
    "        l3 = ax2.scatter([], [], s=20, marker=\"*\", c=\"orange\")\n",
    "        ax2.legend([l1, l2, l3], [\"$\\mathrm{cc} \\geq \\;$\"+f\"{outlierCC:.2f}\", \"$\\mathrm{cc} <\\;$\" +\n",
    "                                  f\"{outlierCC:.2f}\", \"Fault\"], loc=\"lower left\", bbox_to_anchor=(-0.60, 0.0))\n",
    "        fig.savefig(nameBase + \".pdf\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    plotCosineFit()\n",
    "\n",
    "cc(\"us7000blsw\", \"2020-09-07T11:03:17.037000\", 7.641, -37.351, 4.62, \"us7000bivi\", \"2020-09-06T08:29:47.435000\", 7.757, -37.147, 4.52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d77521d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no dash and no transparent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5dc11a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\student\\\\Downloads\\\\OTF2021-1.0.1\\\\cross-correl-sample\\\\six_earthquakes\\\\stations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 221\u001b[0m\n\u001b[0;32m    218\u001b[0m     plotCosineFit()\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m#cc(\"us7000blsw\", \"2020-09-07T11:03:17.037000\", 7.641, -37.351, 4.62, \"us7000bivi\", \"2020-09-06T08:29:47.435000\", 7.757, -37.147, 4.52)\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m \u001b[43mcc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus700054\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2020-09-06 08:24:15\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7.849\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m37.106\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4.479\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus700068\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2020-09-07 00:46:32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7.811\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m37.024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4.479\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m, in \u001b[0;36mcc\u001b[1;34m(id1, t1, lat1, lon1, mag1, id2, t2, lat2, lon2, mag2)\u001b[0m\n\u001b[0;32m     50\u001b[0m content \u001b[38;5;241m=\u001b[39m {x: [] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaLat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaLon\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m     51\u001b[0m nameBase \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(waveDir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcc_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m dfStations \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveDir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstations.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m dfStations\u001b[38;5;241m.\u001b[39mitertuples():\n\u001b[0;32m     55\u001b[0m     waveName1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(id1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m rs\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m rs\u001b[38;5;241m.\u001b[39msta \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.sac\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32md:\\Users\\student\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\student\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\student\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Users\\student\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\student\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\Users\\student\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\student\\\\Downloads\\\\OTF2021-1.0.1\\\\cross-correl-sample\\\\six_earthquakes\\\\stations.csv'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from obspy import read\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy.io.sac.util import SacIOError\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "from pyproj import Geod\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "waveDir = r\"C:\\Users\\student\\Downloads\\OTF2021-1.0.1\\cross-correl-sample\\six_earthquakes\\Fault1\"\n",
    "\n",
    "\n",
    "def cc(id1, t1, lat1, lon1, mag1, id2, t2, lat2, lon2, mag2):\n",
    "    outlierCC = 0.70\n",
    "    outlierDt = 35\n",
    "    numCCToFit = 8\n",
    "    vgroup = 3.75\n",
    "    outlierDx = vgroup * outlierDt\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    def fit_cosine_ls_norm(azi, dx, cc, u0):\n",
    "        mask = np.logical_and(cc >= outlierCC, np.abs(dx) <= outlierDx)\n",
    "        _azi = azi[mask]\n",
    "        _dx = dx[mask]\n",
    "        if len(_azi) < numCCToFit or badCoverage(_azi):\n",
    "            return np.array([np.nan, np.nan, np.nan]), np.array([np.inf, np.inf, np.inf])\n",
    "\n",
    "        res = least_squares(residual_cosine, [0., u0, 0.], args=(\n",
    "            _azi, _dx), loss='soft_l1', jac='3-point')\n",
    "\n",
    "        err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=np.std(\n",
    "            residual_cosine(res.x, _azi, _dx)))\n",
    "        return res.x, err\n",
    "\n",
    "   \n",
    "\n",
    "    wv1, wv2 = [], []\n",
    "    content = {x: [] for x in [\"azi\", \"dt\", \"cc\", \"net\", \"sta\", \"staLat\", \"staLon\"]}\n",
    "    nameBase = os.path.join(waveDir, \"cc_results\")\n",
    "    dfStations = pd.read_csv(os.path.join(waveDir, \"stations.csv\"))\n",
    "\n",
    "    for rs in dfStations.itertuples():\n",
    "        waveName1 = str(id1) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        waveName2 = str(id2) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        wavePath1 = os.path.join(waveDir, waveName1)\n",
    "        wavePath2 = os.path.join(waveDir, waveName2)\n",
    "\n",
    "        if (\n",
    "            np.isnan(rs.dist) or\n",
    "            not os.path.isfile(wavePath1) or\n",
    "            not os.path.isfile(wavePath2) or\n",
    "            os.path.getsize(wavePath1) == 0 or\n",
    "            os.path.getsize(wavePath2) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        midLon, midLat = angularMean([lon1, lon2]), angularMean([lat1, lat2])\n",
    "        fz, _, _ = geod.inv(midLon, midLat, rs.lon, rs.lat)\n",
    "        try:\n",
    "            st1 = read(wavePath1, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath1)\n",
    "            continue\n",
    "        try:\n",
    "            st2 = read(wavePath2, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath2)\n",
    "            continue\n",
    "\n",
    "        if st1[0].stats[\"sampling_rate\"] != st2[0].stats[\"sampling_rate\"]:\n",
    "            maxSamplingRate = max(st1[0].stats[\"sampling_rate\"], st2[0].stats[\"sampling_rate\"])\n",
    "            for ss in [st1, st2]:\n",
    "                if maxSamplingRate != ss[0].stats[\"sampling_rate\"]:\n",
    "                    ss.resample(maxSamplingRate, no_filter=True)\n",
    "\n",
    "        cc = correlate(\n",
    "            st1[0], st2[0], demean=True, normalize='naive',\n",
    "            shift=min(st1[0].stats.npts, st2[0].stats.npts, int(np.round(outlierDt / st1[0].stats.delta * 2.0)))\n",
    "        )\n",
    "        shift, value = xcorr_max(cc, abs_max=False)\n",
    "        dt = st1[0].stats.delta * shift\n",
    "        content['azi'].append(fz)\n",
    "        content['dt'].append(dt)\n",
    "        content['cc'].append(value)\n",
    "        content['net'].append(rs.net)\n",
    "        content['sta'].append(rs.sta)\n",
    "        content['staLat'].append(rs.lat)\n",
    "        content['staLon'].append(rs.lon)\n",
    "        wv1.append(st1[0].data)\n",
    "        wv2.append(st2[0].data)\n",
    "\n",
    "    pd.DataFrame(content).to_csv(nameBase + \".csv\", index=False)\n",
    "    if len(content[\"dt\"]) < numCCToFit:\n",
    "        return\n",
    "\n",
    "    yy = np.array(content['dt']) * vgroup\n",
    "    xx = np.array(content['azi'])\n",
    "    _, _, u0 = geod.inv(lon1, lat1, lon2, lat2)\n",
    "    popt, pcov = fit_cosine_ls_norm(xx, yy, np.array(content['cc']), np.abs(u0 / 1e3))\n",
    "    A, B, C = popt\n",
    "    dA, dB, dC = pcov\n",
    "    highcc = list(filter(lambda x: np.abs(x) >= outlierCC, content['cc']))\n",
    "    summary = {\n",
    "        'A': popt[0], 'B': popt[1], 'C': popt[2],\n",
    "        'dA': pcov[0], 'dB': pcov[1], 'dC': pcov[2],\n",
    "        'numHighCC': len(highcc),\n",
    "    }\n",
    "    with open(nameBase + \".json\", \"w\") as fp:\n",
    "        json.dump(summary, fp, indent=4)\n",
    "\n",
    "    def plotCosineFit():\n",
    "        fig = plt.figure(figsize=(8, 10))\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[2, 1])\n",
    "        ax0 = fig.add_subplot(gs[0, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(gs[1, 0], projection=ccrs.AzimuthalEquidistant(lon1, lat1))\n",
    "        ax2.stock_img()\n",
    "        ax2.scatter(lon1, lat1, s=20, marker=\"*\", c=\"orange\", transform=ccrs.PlateCarree(), zorder=2)\n",
    "\n",
    "        ax3 = fig.add_axes([0.55, 0.89, 0.1, 0.1])\n",
    "        ax3.set_aspect(\"equal\")\n",
    "        bc1 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.75), width=0.4)\n",
    "       \n",
    "        bc2 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.25), width=0.4)\n",
    "        ax3.add_collection(bc1)\n",
    "        ax3.add_collection(bc2)\n",
    "        ax3.axis('off')\n",
    "\n",
    "        for i in range(len(wv1)):\n",
    "            y1 = np.arange(0.0, len(wv1[i])) * st1[0].stats.delta\n",
    "            y2 = np.arange(0.0, len(wv2[i])) * st2[0].stats.delta\n",
    "            w1 = wv1[i] - np.nanmean(wv1[i])\n",
    "            w2 = wv2[i] - np.nanmean(wv2[i])\n",
    "            w1 = w1 / np.nanmax(np.abs(w1)) * 8\n",
    "            w2 = w2 / np.nanmax(np.abs(w2)) * 8\n",
    "\n",
    "            isoutlier = not (np.abs(content[\"dt\"][i]) <= outlierDt and np.abs(content[\"cc\"][i]) >= outlierCC)\n",
    "            #linestyle = '--' if isoutlier else '-' \n",
    "            #alpha = 0.2 if isoutlier else 1.0\n",
    "            linestyle = '-'  # Remove dashed line\n",
    "            alpha = 1.0  # Remove transparency\n",
    "            fc = 'white' if isoutlier else 'tab:green'\n",
    "            ax0.plot(y1, w1 + content[\"azi\"][i], color=\"tab:blue\", linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax0.plot(y2, w2 + content[\"azi\"][i], color=\"tab:red\", linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax1.scatter(content[\"dt\"][i], content[\"azi\"][i], fc=fc, s=15, marker=\"o\", ec=\"black\", zorder=2)\n",
    "\n",
    "            trajectorColor = \"gray\" if isoutlier else \"tab:pink\"\n",
    "            #trajectorStyle = \"--\" if isoutlier else \"-\"\n",
    "            trajectorStyle = \"-\"  # Remove dashed line\n",
    "            ax2.plot([lon1, content[\"staLon\"][i]], [lat1, content[\"staLat\"][i]],\n",
    "                     color=trajectorColor, transform=ccrs.Geodetic(), zorder=1, linestyle=trajectorStyle)\n",
    "\n",
    "        xx = np.arange(-180, 180, 1.0)\n",
    "        if np.isnan(A):\n",
    "            yy = np.array([np.nan for _ in xx])\n",
    "            fittingColor = \"gray\"\n",
    "        else:\n",
    "            yy = A + B * np.cos(np.deg2rad(xx - C))\n",
    "            yy /= vgroup\n",
    "            fittingColor = \"tab:orange\"\n",
    "\n",
    "        ax1.plot(yy, xx, c=fittingColor, zorder=1)\n",
    "        _b, _c = B, C\n",
    "        if _b < 0:\n",
    "            _b *= -1\n",
    "            _c += 180\n",
    "        _c %= 360\n",
    "        ax1.text(0.5, 1.05, \"$\\Delta x = \\;$\" + f\"{_b:.0f}\" + \"$\\;\\pm \\;$\" + f\"{dB:.0f}\" + \"$\\;\\mathrm{km}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax1.text(0.5, 1.12, r\"$\\theta = \\;$\" + f\"{_c:.1f}\" + \"$\\;\\pm \\;$\" + f\"{dC:.1f}\" + \"$^{\\circ}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax0.set_ylim([-180, 180])\n",
    "        ax1.set_ylim([-180, 180])\n",
    "        ax1.set_xlim([-outlierDt, outlierDt])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.grid(True, which=\"major\", color='gray', linestyle='--', linewidth=1)\n",
    "        ax1.set_xlabel(\"$\\Delta T$ (second)\")\n",
    "        ax0.set_xlabel(\"Time (second)\")\n",
    "        ax0.set_ylabel(\"Azimuth (deg)\")\n",
    "        ax0.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "        l1, = ax0.plot([], [], color=\"tab:blue\")\n",
    "        l2, = ax0.plot([], [], color=\"tab:red\")\n",
    "        _tstr1 = datetime.fromisoformat(t1)\n",
    "        _tstr1 = _tstr1.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag1:.1f}')\n",
    "        _tstr2 = datetime.fromisoformat(t2)\n",
    "        _tstr2 = _tstr2.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag2:.1f}')\n",
    "        ax0.legend([l1, l2], [_tstr1, _tstr2], loc=\"upper center\", bbox_to_anchor=(0.5, 1.18))\n",
    "\n",
    "        l1, = ax1.plot([], [], color=\"tab:orange\")\n",
    "        l2 = ax0.scatter([], [], fc='tab:green', s=15, marker=\"o\", ec=\"black\")\n",
    "        l3 = ax0.scatter([], [], fc='white', s=15, marker=\"o\", ec=\"black\")\n",
    "        ax1.legend([l1, l2, l3], [\"Cosine Fitting\", \"Observation (cc $\\geq \\;$\" +\n",
    "                                  f\"{outlierCC:.2f})\", f\"Observation (cc $<$ {outlierCC:.2f})\"], loc=\"lower center\", bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "        l1, = ax2.plot([], [], color=\"tab:pink\", linestyle=\"-\")\n",
    "        l2, = ax2.plot([], [], color=\"gray\", linestyle=\"--\")\n",
    "        l3 = ax2.scatter([], [], s=20, marker=\"*\", c=\"orange\")\n",
    "        ax2.legend([l1, l2, l3], [\"$\\mathrm{cc} \\geq \\;$\"+f\"{outlierCC:.2f}\", \"$\\mathrm{cc} <\\;$\" +\n",
    "                                  f\"{outlierCC:.2f}\", \"Events\"], loc=\"lower left\", bbox_to_anchor=(-0.60, 0.0))\n",
    "        fig.savefig(nameBase + \"2\" + \".pdf\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    plotCosineFit()\n",
    "\n",
    "#cc(\"us7000blsw\", \"2020-09-07T11:03:17.037000\", 7.641, -37.351, 4.62, \"us7000bivi\", \"2020-09-06T08:29:47.435000\", 7.757, -37.147, 4.52)\n",
    "cc(\"us700054\",\"2020-09-06 08:24:15\",7.849,-37.106,4.479,\"us700068\",\"2020-09-07 00:46:32\",7.811,-37.024,4.479)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import operator\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque, namedtuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nlopt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from obspy import read\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy.io.sac.util import SacIOError\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "from pyproj import Geod\n",
    "from scipy.optimize import curve_fit, least_squares, minimize\n",
    "\n",
    "from GetData import AbstractFaultProcess\n",
    "from utils import *\n",
    "\n",
    "def cc(fa, id1, t1, lat1, lon1, mag1, id2, t2, lat2, lon2, mag2):\n",
    "    print(f\"Processing pair: {id1}-{id2}\")\n",
    "    # cannot serialize pandas frame, which sliently quits multiprocessor\n",
    "\n",
    "    outlierCC = fa.relocationConfig.get(\"outlierCC\", 0.70)\n",
    "    outlierDt = fa.relocationConfig.get(\"outlierDt\", 35)\n",
    "    numCCToFit = fa.relocationConfig.get(\"numCCToFit\", 8)\n",
    "    vgroup = fa.relocationConfig.get(\"vgroup\", 3.75)\n",
    "    outlierDx = vgroup * outlierDt\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "    def badCoverage(azi, maxgap=120.0):  # exclude those with azimuth gap larger than `maxgap`\n",
    "        if len(azi) == 0:\n",
    "            return False\n",
    "        _azi = sorted([x % 360 for x in azi])\n",
    "        diff = [_azi[i+1] - _azi[i] for i in range(len(_azi) - 1)]\n",
    "        diff.append(360 - _azi[-1] + _azi[0])\n",
    "        return np.any(np.array(diff) > maxgap)\n",
    "\n",
    "    def residual_cosine(x, azi, dt):\n",
    "        return x[0] + x[1] * np.cos(np.deg2rad(azi - x[2])) - dt\n",
    "\n",
    "    def fit_cosine_ls_norm(azi, dx, cc, u0):\n",
    "        mask = np.logical_and(cc >= outlierCC, np.abs(dx) <= outlierDx)\n",
    "        _azi = azi[mask]\n",
    "        _dx = dx[mask]\n",
    "        if len(_azi) < numCCToFit or badCoverage(_azi):\n",
    "            return np.array([np.nan, np.nan, np.nan]), np.array([np.inf, np.inf, np.inf])\n",
    "\n",
    "        # initial solution affect the local minimum obtained\n",
    "        res = least_squares(residual_cosine, [0., u0, 0.], args=(\n",
    "            _azi, _dx), loss='soft_l1', jac='3-point')\n",
    "\n",
    "        # On way of uncertainty, but inappropriate since we use other forms of norm\n",
    "        # https://stackoverflow.com/a/21844726/8697614\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # pcov = cov * (res.fun ** 2).sum() / (len(_azi) - len(res.x)) # reduced chi-square distribution\n",
    "        # err = np.sqrt(np.diagonal(pcov))\n",
    "\n",
    "        # Another way of uncertainty, still assume your function value is chi-square\n",
    "        # https://stackoverflow.com/a/53489234/8697614, notice `dx[2] can exceed 360`\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # err2 = np.sqrt(np.diagonal(cov) * np.abs(res.cost))\n",
    "        # return res.x, err\n",
    "\n",
    "        # The third way: bootstrap\n",
    "        # This is what McGuire did, assuming 1s error in dt\n",
    "        # err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=1.0 * vgroup)\n",
    "        # we instead use fitting residual as the error for bootstrap\n",
    "        err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=np.std(\n",
    "            residual_cosine(res.x, _azi, _dx)))\n",
    "        return res.x, err\n",
    "\n",
    "    def bootstrap_uncertainty(x, y, p0, numiters=400, stderr=3.75, nsigma=1.0):\n",
    "        x_cmt, x_dist, x_azi = [], [], []\n",
    "        n = len(y)\n",
    "        for i in range(numiters):\n",
    "            yy = y + np.random.default_rng().normal(0.0, stderr, n)\n",
    "            res = least_squares(residual_cosine, p0, args=(\n",
    "                x, yy), loss='soft_l1', jac='3-point')\n",
    "            x_cmt.append(res.x[0])\n",
    "            x_dist.append(res.x[1])\n",
    "            x_azi.append(res.x[2])\n",
    "        return nsigma*np.std(x_cmt), nsigma*np.std(x_dist), nsigma*np.std(x_azi)\n",
    "\n",
    "    fm1, fm2 = fa.fm[str(id1)][\"np1\"], fa.fm[str(id2)][\"np1\"]\n",
    "    wv1, wv2 = [], []\n",
    "    content = {x: [] for x in [\"azi\", \"dt\",\n",
    "                               \"cc\", \"net\", \"sta\", \"staLat\", \"staLon\"]}\n",
    "    nameBase = os.path.join(\n",
    "        fa.ccDir, '-'.join(map(str, [id1, id2])))\n",
    "    dfStations = pd.read_csv(os.path.join(fa.dir, \"stations.csv\"))\n",
    "    for rs in dfStations.itertuples():\n",
    "        waveName1 = str(id1) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        waveName2 = str(id2) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        wavePath1 = os.path.join(fa.waveDir, waveName1)\n",
    "        wavePath2 = os.path.join(fa.waveDir, waveName2)\n",
    "\n",
    "        if (\n",
    "            np.isnan(rs.dist) or\n",
    "            not os.path.isfile(wavePath1) or\n",
    "            not os.path.isfile(wavePath2) or\n",
    "            os.path.getsize(wavePath1) == 0 or\n",
    "            os.path.getsize(wavePath2) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        midLon, midLat = angularMean(\n",
    "            [lon1, lon2]), angularMean([lat1, lat2])\n",
    "        fz, _, _ = geod.inv(midLon, midLat, rs.lon,\n",
    "                            rs.lat)  # fz: (-180, 180)\n",
    "        try:\n",
    "            st1 = read(wavePath1, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath1)\n",
    "            continue\n",
    "        try:\n",
    "            st2 = read(wavePath2, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath2)\n",
    "            continue\n",
    "\n",
    "        if st1[0].stats[\"sampling_rate\"] != st2[0].stats[\"sampling_rate\"]:\n",
    "            maxSamplingRate = max(st1[0].stats[\"sampling_rate\"], st2[0].stats[\"sampling_rate\"])\n",
    "            for ss in [st1, st2]:\n",
    "                if maxSamplingRate != ss[0].stats[\"sampling_rate\"]:\n",
    "                    ss.resample(maxSamplingRate, no_filter=True)\n",
    "\n",
    "        cc = correlate(\n",
    "            st1[0], st2[0], demean=True, normalize='naive',\n",
    "            shift=min(st1[0].stats.npts, st2[0].stats.npts, int(\n",
    "                np.round(outlierDt / st1[0].stats.delta * 2.0)))\n",
    "        )\n",
    "        # consider only positive cc\n",
    "        shift, value = xcorr_max(cc, abs_max=False)\n",
    "        dt = st1[0].stats.delta * shift\n",
    "        content['azi'].append(fz)\n",
    "        content['dt'].append(dt)\n",
    "        content['cc'].append(value)\n",
    "        content['net'].append(rs.net)\n",
    "        content['sta'].append(rs.sta)\n",
    "        content['staLat'].append(rs.lat)\n",
    "        content['staLon'].append(rs.lon)\n",
    "        wv1.append(st1[0].data)\n",
    "        wv2.append(st2[0].data)\n",
    "\n",
    "    pd.DataFrame(content).to_csv(nameBase + \".csv\", index=False)\n",
    "    if len(content[\"dt\"]) < numCCToFit:\n",
    "        return\n",
    "\n",
    "    yy = np.array(content['dt']) * vgroup\n",
    "    xx = np.array(content['azi'])\n",
    "    # initial guess of distance, important for least square fit\n",
    "    _, _, u0 = geod.inv(lon1, lat1, lon2, lat2)\n",
    "    popt, pcov = fit_cosine_ls_norm(\n",
    "        xx, yy, np.array(content['cc']), np.abs(u0 / 1e3))\n",
    "    A, B, C = popt\n",
    "    dA, dB, dC = pcov\n",
    "    highcc = list(filter(lambda x: np.abs(x) >= outlierCC, content['cc']))\n",
    "    summary = {\n",
    "        'A': popt[0], 'B': popt[1], 'C': popt[2],\n",
    "        'dA': pcov[0], 'dB': pcov[1], 'dC': pcov[2],\n",
    "        'numHighCC': len(highcc),\n",
    "    }\n",
    "    with open(nameBase + \".json\", \"w\") as fp:\n",
    "        json.dump(summary, fp, indent=4)\n",
    "\n",
    "    def plotCosineFit():\n",
    "        fig = plt.figure(figsize=(8, 10))\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[2, 1])\n",
    "        ax0 = fig.add_subplot(gs[0, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(\n",
    "            gs[1, 0], projection=ccrs.AzimuthalEquidistant(lon1, lat1))\n",
    "        ax2.stock_img()\n",
    "        ax2.scatter(lon1, lat1, s=20, marker=\"*\", c=\"orange\",\n",
    "                    transform=ccrs.PlateCarree(), zorder=2)\n",
    "\n",
    "        ax3 = fig.add_axes([0.55, 0.89, 0.1, 0.1])\n",
    "        ax3.set_aspect(\"equal\")\n",
    "        if np.isnan(fm1[0]):\n",
    "            bc1 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.75), width=0.4)\n",
    "        else:\n",
    "            bc1 = beach(fm1, facecolor=\"tab:blue\", xy=(0.5, 0.75), width=0.4)\n",
    "        if np.isnan(fm2[0]):\n",
    "            bc2 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.25), width=0.4)\n",
    "        else:\n",
    "            bc2 = beach(fm2, facecolor=\"tab:red\", xy=(0.5, 0.25), width=0.4)\n",
    "        ax3.add_collection(bc1)\n",
    "        ax3.add_collection(bc2)\n",
    "        ax3.axis('off')\n",
    "\n",
    "        for i in range(len(wv1)):\n",
    "            # do not set `step=delta` which may cause length mismatch\n",
    "            y1 = np.arange(0.0, len(wv1[i])) * st1[0].stats.delta\n",
    "            y2 = np.arange(0.0, len(wv2[i])) * st2[0].stats.delta\n",
    "            w1 = wv1[i] - np.nanmean(wv1[i])\n",
    "            w2 = wv2[i] - np.nanmean(wv2[i])\n",
    "            w1 = w1 / np.nanmax(np.abs(w1)) * 8\n",
    "            w2 = w2 / np.nanmax(np.abs(w2)) * 8\n",
    "\n",
    "            isoutlier = not (np.abs(content[\"dt\"][i]) <= outlierDt and np.abs(\n",
    "                content[\"cc\"][i]) >= outlierCC)\n",
    "            linestyle = '--' if isoutlier else '-'\n",
    "            alpha = 0.2 if isoutlier else 1.0\n",
    "            fc = 'white' if isoutlier else 'tab:green'\n",
    "            ax0.plot(y1, w1 + content[\"azi\"][i], color=\"tab:blue\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax0.plot(y2, w2 + content[\"azi\"][i], color=\"tab:red\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax1.scatter(content[\"dt\"][i], content[\"azi\"][i],\n",
    "                        fc=fc, s=15, marker=\"o\", ec=\"black\", zorder=2)\n",
    "\n",
    "            trajectorColor = \"gray\" if isoutlier else \"tab:pink\"\n",
    "            trajectorStyle = \"--\" if isoutlier else \"-\"\n",
    "            ax2.plot([lon1, content[\"staLon\"][i]], [lat1, content[\"staLat\"][i]],\n",
    "                     color=trajectorColor, transform=ccrs.Geodetic(), zorder=1, linestyle=trajectorStyle)\n",
    "\n",
    "        xx = np.arange(-180, 180, 1.0)\n",
    "        if np.isnan(A):\n",
    "            yy = np.array([np.nan for _ in xx])\n",
    "            fittingColor = \"gray\"\n",
    "        else:\n",
    "            yy = A + B * np.cos(np.deg2rad(xx - C))\n",
    "            yy /= vgroup\n",
    "            fittingColor = \"tab:orange\"\n",
    "\n",
    "        ax1.plot(yy, xx, c=fittingColor, zorder=1)\n",
    "        _b, _c = B, C\n",
    "        if _b < 0:\n",
    "            _b *= -1\n",
    "            _c += 180\n",
    "        _c %= 360\n",
    "        ax1.text(0.5, 1.05, \"$\\Delta x = \\;$\" + f\"{_b:.0f}\" + \"$\\;\\pm \\;$\" + f\"{dB:.0f}\" + \"$\\;\\mathrm{km}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax1.text(0.5, 1.12, r\"$\\theta = \\;$\" + f\"{_c:.1f}\" + \"$\\;\\pm \\;$\" + f\"{dC:.1f}\" + \"$^{\\circ}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax0.set_ylim([-180, 180])\n",
    "        ax1.set_ylim([-180, 180])\n",
    "        ax1.set_xlim([-outlierDt, outlierDt])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.grid(True, which=\"major\", color='gray',\n",
    "                 linestyle='--', linewidth=1)\n",
    "        ax1.set_xlabel(\"$\\Delta T$ (second)\")\n",
    "        ax0.set_xlabel(\"Time (second)\")\n",
    "        ax0.set_ylabel(\"Azimuth (deg)\")\n",
    "        ax0.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "        l1, = ax0.plot([], [], color=\"tab:blue\")\n",
    "        l2, = ax0.plot([], [], color=\"tab:red\")\n",
    "        _tstr1 = datetime.fromisoformat(t1)\n",
    "        _tstr1 = _tstr1.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag1:.1f}')\n",
    "        _tstr2 = datetime.fromisoformat(t2)\n",
    "        _tstr2 = _tstr2.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag2:.1f}')\n",
    "        ax0.legend([l1, l2], [_tstr1, _tstr2],\n",
    "                   loc=\"upper center\", bbox_to_anchor=(0.5, 1.18))\n",
    "\n",
    "        l1, = ax1.plot([], [], color=\"tab:orange\")\n",
    "        l2 = ax0.scatter([], [], fc='tab:green', s=15, marker=\"o\", ec=\"black\")\n",
    "        l3 = ax0.scatter([], [], fc='white', s=15, marker=\"o\", ec=\"black\")\n",
    "        ax1.legend([l1, l2, l3], [\"Cosine Fitting\", \"Observation (cc $\\geq \\;$\" +\n",
    "                                  f\"{outlierCC:.2f})\", f\"Observation (cc $<$ {outlierCC:.2f})\"], loc=\"lower center\", bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "        l1, = ax2.plot([], [], color=\"tab:pink\", linestyle=\"-\")\n",
    "        l2, = ax2.plot([], [], color=\"gray\", linestyle=\"--\")\n",
    "        l3 = ax2.scatter([], [], s=20, marker=\"*\", c=\"orange\")\n",
    "        ax2.legend([l1, l2, l3], [\"$\\mathrm{cc} \\geq \\;$\"+f\"{outlierCC:.2f}\", \"$\\mathrm{cc} <\\;$\" +\n",
    "                                  f\"{outlierCC:.2f}\", fa.name], loc=\"lower left\", bbox_to_anchor=(-0.60, 0.0))\n",
    "        fig.savefig(nameBase + \".pdf\")\n",
    "        print(f\"Saved {os.path.basename(nameBase)}.\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    plotCosineFit()\n",
    "\n",
    "\n",
    "def crossCorrelate(fa):\n",
    "    dfPairss = pd.read_csv(os.path.join(fa.dir, \"catalog-pair.csv\"))\n",
    "    total_pairs = dfPairss.shape[0]\n",
    "    processed_pairs = 0\n",
    "\n",
    "    ccFiles = [os.path.join(fa.ccDir, x) for x in os.listdir(fa.ccDir)]\n",
    "    for f in ccFiles:\n",
    "        os.remove(f)\n",
    "    with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "        futures = []\n",
    "        for r in dfPairss.itertuples(index=True):\n",
    "            futures.append(executor.submit(cc, fa, r.id1, r.t1, r.lat1,\n",
    "                                           r.lon1, r.mag1, r.id2, r.t2, r.lat2, r.lon2, r.mag2))\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "            processed_pairs += 1\n",
    "            if processed_pairs % 10 == 0:\n",
    "                print(f\"Processed {processed_pairs}/{total_pairs} pairs\")\n",
    "    print(\"Cross-correlation completed.\")\n",
    "\n",
    "class RelocationProcedure(AbstractFaultProcess):\n",
    "\n",
    "    def __init__(self, name, *args, **kwargs):\n",
    "        super().__init__(name)\n",
    "        self.waveDir = os.path.join(self.dir, \"waves\")\n",
    "        if not os.path.isdir(self.waveDir):\n",
    "            os.mkdir(self.waveDir)\n",
    "        self.ccDir = os.path.join(self.dir, \"cc\")\n",
    "        if not os.path.isdir(self.ccDir):\n",
    "            os.mkdir(self.ccDir)\n",
    "        with open(os.path.join(self.dir, \"mt.json\")) as fp:\n",
    "            self.fm = json.load(fp)\n",
    "        self.relocationConfig = kwargs\n",
    "\n",
    "\n",
    "def optimize(fa):\n",
    "\n",
    "    def formEdge(x: dict, linkNum: int = 8):\n",
    "        return \\\n",
    "            (not np.isnan(x['B'])) and \\\n",
    "            (not np.isinf(x['dB'])) and \\\n",
    "            x['numHighCC'] >= linkNum\n",
    "\n",
    "    def buildPairGraph():\n",
    "        dfPairs = pd.read_csv(os.path.join(fa.dir, \"catalog-pair.csv\"))\n",
    "        files = (x for x in os.listdir(fa.ccDir) if x.endswith(\".json\"))\n",
    "        pairkey2row = {(r.id1, r.id2): r for r in dfPairs.itertuples()}\n",
    "\n",
    "        G = nx.Graph()\n",
    "        for file in files:\n",
    "            with open(os.path.join(fa.ccDir, file), \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                if formEdge(data):\n",
    "                    info = os.path.splitext(file)[0].split('-')\n",
    "                    id1, id2 = map(str, info)\n",
    "                    r = pairkey2row.get((id1, id2), None)\n",
    "                    if not r:\n",
    "                        continue\n",
    "                    if id1 != r.id1 or id2 != r.id2:\n",
    "                        raise ValueError(\"pair id mismatch.\")\n",
    "\n",
    "                    # duplicate hashing is ignored\n",
    "                    G.add_node(id1,\n",
    "                               lat=r.lat1, lon=r.lon1, mag=r.mag1,\n",
    "                               t=r.t1, group=-1)\n",
    "                    G.add_node(id2,\n",
    "                               lat=r.lat2, lon=r.lon2, mag=r.mag2,\n",
    "                               t=r.t2, group=-1)\n",
    "                    G.add_edge(id1, id2,\n",
    "                               B=data['B'], C=data['C'],\n",
    "                               dB=data['dB'], dC=data['dC'])\n",
    "        return G\n",
    "\n",
    "    def traverseGraph(G, relocateOneWay=True):\n",
    "        # BFS, relocate one group of events by single reference abiding shortest path\n",
    "        # This, however, does not account for discrepancy with direct observation and\n",
    "        # new locations. Using optimization is perferred, see below.\n",
    "\n",
    "        # If not relocate, it aims to idenfity groups and label accordingly.\n",
    "        def traverse(n):\n",
    "            queueSet.add(n)\n",
    "            for k in G.adj[n].keys():\n",
    "                if (not visited[k]) and (k not in queueSet):\n",
    "                    if relocateOneWay:\n",
    "                        B, C = G.adj[n][k]['B'], G.adj[n][k]['C']\n",
    "                        # all pairs id1 is more recent than id2\n",
    "                        C = C if IDTimePair[n] > IDTimePair[k] else C + 180.0  # only change `B` or `C`, not both\n",
    "                        lat1, lon1 = G.nodes.data(\n",
    "                            'lat')[n], G.nodes.data('lon')[n]\n",
    "                        lon2, lat2, _ = geod.fwd(lon1, lat1, C, B*1e3)\n",
    "                        G.nodes[k].update(lat=lat2, lon=lon2)\n",
    "                    visited[k] = True\n",
    "                    IDTimePairDynamic.pop(k, None)\n",
    "                    G.nodes[k].update(group=groupid)\n",
    "                    Q.append(k)\n",
    "                    queueSet.add(k)\n",
    "\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        Q = deque()\n",
    "        queueSet = set()\n",
    "        visited = dict(G.nodes.data('visited'))\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "        IDTimePairDynamic = dict(G.nodes.data(\"t\"))\n",
    "        groupid = 1\n",
    "\n",
    "        while count := sum([1 for (k, v) in visited.items() if not v]) > 0:\n",
    "            # use newest event as reference\n",
    "            startID = max(IDTimePairDynamic, key=IDTimePairDynamic.get)\n",
    "            Q.append(startID)\n",
    "            visited[startID] = True\n",
    "            IDTimePairDynamic.pop(startID)\n",
    "            G.nodes[startID].update(group=groupid)\n",
    "            while Q:\n",
    "                traverse(Q.popleft())\n",
    "            groupid += 1\n",
    "        return G\n",
    "\n",
    "    def traverseOptimizeGraph(G):\n",
    "        Q = deque()\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        optimized = dict(G.nodes.data('visited'))\n",
    "        queueSet = set()\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "        IDTimePair2 = dict(G.nodes.data(\"t\"))\n",
    "\n",
    "        def optimizeNodeLocation(node):\n",
    "\n",
    "            def objective(x, lons, lats, B, C, dB, dC):\n",
    "                res = 0.0\n",
    "                for i in range(len(lons)):\n",
    "                    fz, _, dist = geod.inv(x[0], x[1], lons[i], lats[i])\n",
    "                    fz, dist = _normalize_fz_dist(fz, dist)\n",
    "                    res += np.abs(angularDiff(fz, C[i]) / dC[i]) + \\\n",
    "                        np.abs((dist/1e3 - B[i]) / dB[i])\n",
    "                return res\n",
    "\n",
    "            ks = G.adj[node].keys()\n",
    "            queueSet.add(node)\n",
    "            IDTimePair.pop(node, None)\n",
    "            for k in ks:\n",
    "                if (not optimized[k]) and (k not in queueSet):\n",
    "                    Q.append(k)\n",
    "                    queueSet.add(k)\n",
    "                    IDTimePair.pop(k, None)\n",
    "\n",
    "            lons = np.array([G.nodes.data('lon')[k] for k in ks])\n",
    "            lats = np.array([G.nodes.data('lat')[k] for k in ks])\n",
    "            x0 = np.array([angularMean(lons), angularMean(lats)])\n",
    "            Bs = np.array([G.adj[node][k]['B'] for k in ks])\n",
    "            Cs = np.array([G.adj[node][k]['C'] for k in ks])\n",
    "            dBs = np.array([G.adj[node][k]['dB'] for k in ks])\n",
    "            dCs = np.array([G.adj[node][k]['dC'] for k in ks])\n",
    "            for i, k in enumerate(ks):\n",
    "                # all pairs id1 is more recent than id2\n",
    "                if IDTimePair2[node] < IDTimePair2[k]:  # correction for direction\n",
    "                    Cs[i] += 180.0\n",
    "                if Bs[i] < 0:  # correction for negative distance\n",
    "                    Bs[i] *= -1\n",
    "                    Cs[i] += 180.0\n",
    "                Cs[i] %= 360  # correction for azimuth angle between [0, 360)\n",
    "            res = minimize(objective, x0, args=(\n",
    "                lons, lats, Bs, Cs, dBs, dCs), method=\"Nelder-Mead\")\n",
    "            G.nodes[node].update(lat=res.x[1], lon=res.x[0])\n",
    "            optimized[node] = True\n",
    "\n",
    "        while count := sum([1 for (k, v) in optimized.items() if not v]) > 0:\n",
    "            # startID = min([k for (k, v) in optimized.items() if not v])\n",
    "            startID = max(IDTimePair, key=IDTimePair.get)\n",
    "            Q.append(startID)\n",
    "            IDTimePair.pop(startID)\n",
    "            while Q:\n",
    "                optimizeNodeLocation(Q.popleft())\n",
    "        return G\n",
    "\n",
    "    def optimizeLocation(G, relocateTwoWay=True, relocateTwoWayIter=5, relocateGlobal=True):\n",
    "        if relocateTwoWay:\n",
    "            for _ in range(relocateTwoWayIter):\n",
    "                traverseOptimizeGraph(G)\n",
    "\n",
    "        data = {k: G.nodes[k] for k in G.nodes}\n",
    "        uniqueGroupID = set([data[k]['group'] for k in data.keys()])\n",
    "        linkContent = {x: [] for x in [\n",
    "            \"id1\", \"t1\", \"lat1\", \"lon1\", \"mag1\",\n",
    "            \"id2\", \"t2\", \"lat2\", \"lon2\", \"mag2\",\n",
    "            \"dist\", \"B\", \"dB\",\n",
    "            \"azi\", \"C\", \"dC\",\n",
    "            \"group\", \"weight\",\n",
    "        ]}\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "\n",
    "        for gid in uniqueGroupID:\n",
    "            es = [x for x in G.edges if data[x[0]]['group'] == gid]\n",
    "            es.sort(key=lambda x: (IDTimePair[x[0]], IDTimePair[x[1]]), reverse=True)\n",
    "            pks = list(set(itertools.chain(*es)))\n",
    "            pks.sort(key=lambda x: IDTimePair[x], reverse=True)\n",
    "            id2t = {k: IDTimePair[k] for k in pks}\n",
    "            masterId = max(id2t, key=id2t.get)\n",
    "            masterLat, masterLon = data[masterId]['lat'], data[masterId]['lon']\n",
    "            key2id = {pks[i]: i for i in range(len(pks))}\n",
    "            Bs = [G.edges[x]['B'] for x in es]\n",
    "            Cs = [G.edges[x]['C'] for x in es]\n",
    "            dBs = [G.edges[x]['dB'] for x in es]\n",
    "            dCs = [G.edges[x]['dC'] for x in es]\n",
    "            lat0 = np.array([data[x]['lat'] for x in pks])\n",
    "            lon0 = np.array([data[x]['lon'] for x in pks])\n",
    "            u0 = [[lat0[i], lon0[i]] for i in range(len(pks))]\n",
    "            u0 = list(itertools.chain(*u0))\n",
    "            u0.append(1)  # initial weight\n",
    "\n",
    "            if not relocateGlobal:\n",
    "                x = u0\n",
    "            else:\n",
    "                avglat0, avglon0 = angularMean(lat0), angularMean(lon0)\n",
    "                searchRange = 10.0  # within 10 degree, should be sufficient\n",
    "                # not rigorously right,\n",
    "                # but we don't have events at polar region or date changing line\n",
    "                lblat = np.clip(lat0 - searchRange, -90, 90)\n",
    "                ublat = np.clip(lat0 + searchRange, -90, 90)\n",
    "                lblon = np.clip(lon0 - searchRange, -180, 180)\n",
    "                ublon = np.clip(lon0 + searchRange, -180, 180)\n",
    "\n",
    "                lb = [[x, y] for x, y in zip(lblat, lblon)]\n",
    "                lb = list(itertools.chain(*lb))\n",
    "                lb.append(0.2)  # lower bound for relative weight\n",
    "                ub = [[x, y] for x, y in zip(ublat, ublon)]\n",
    "                ub = list(itertools.chain(*ub))\n",
    "                ub.append(2)  # upper bound for relative weight\n",
    "\n",
    "                objective, masterLatitudeConstraint, masterLongitudeConstraint = \\\n",
    "                    objectiveFuncFactory(\n",
    "                        masterId, masterLat, masterLon, Bs, Cs, dBs, dCs, key2id, es)\n",
    "\n",
    "                opt = nlopt.opt(nlopt.LN_COBYLA, 2*len(pks)+1)\n",
    "                opt.set_min_objective(objective)\n",
    "                opt.add_equality_constraint(masterLatitudeConstraint, 1e-6)\n",
    "                opt.add_equality_constraint(masterLongitudeConstraint, 1e-6)\n",
    "                opt.set_lower_bounds(lb)\n",
    "                opt.set_upper_bounds(ub)\n",
    "                opt.set_xtol_rel(1e-6)\n",
    "                x = opt.optimize(u0)\n",
    "\n",
    "            for e in es:\n",
    "                _add_link_content(linkContent, G, e, x, key2id, geod, IDTimePair)\n",
    "\n",
    "        pd.DataFrame(linkContent).to_csv(os.path.join(\n",
    "            fa.dir, \"catalog-link.csv\"), index=False)\n",
    "        content = {'id': [], 'time': [], 'lat': [],\n",
    "                   'lon': [], 'mag': [], 'group': []}\n",
    "        content['id'].extend(linkContent['id1'])\n",
    "        content['id'].extend(linkContent['id2'])\n",
    "        content['time'].extend(linkContent['t1'])\n",
    "        content['time'].extend(linkContent['t2'])\n",
    "        content['lat'].extend(linkContent['lat1'])\n",
    "        content['lat'].extend(linkContent['lat2'])\n",
    "        content['lon'].extend(linkContent['lon1'])\n",
    "        content['lon'].extend(linkContent['lon2'])\n",
    "        content['mag'].extend(linkContent['mag1'])\n",
    "        content['mag'].extend(linkContent['mag2'])\n",
    "        content['group'].extend(linkContent['group'])\n",
    "        content['group'].extend(linkContent['group'])\n",
    "        pd.DataFrame(content).drop_duplicates(subset=['id'], keep='last').sort_values(\n",
    "            by=\"time\").to_csv(os.path.join(fa.dir, \"catalog-relocated.csv\"), index=False)\n",
    "\n",
    "    def _add_link_content(linkContent, G, e, sol, key2id, geod, IDTimePair):\n",
    "        # repeated code\n",
    "        # all pairs id1 is more recent than id2\n",
    "        if IDTimePair[e[0]] < IDTimePair[e[1]]:\n",
    "            e = (e[1], e[0])\n",
    "        id1, id2 = key2id[e[0]], key2id[e[1]]\n",
    "\n",
    "        lat1, lon1 = sol[2*id1], sol[2*id1+1]\n",
    "        lat2, lon2 = sol[2*id2], sol[2*id2+1]\n",
    "\n",
    "        linkContent['id1'].append(e[0])\n",
    "        linkContent['t1'].append(G.nodes[e[0]]['t'])\n",
    "        linkContent['lat1'].append(lat1)\n",
    "        linkContent['lon1'].append(lon1)\n",
    "        linkContent['mag1'].append(G.nodes[e[0]]['mag'])\n",
    "        linkContent['id2'].append(e[1])\n",
    "        linkContent['t2'].append(G.nodes[e[1]]['t'])\n",
    "        linkContent['lat2'].append(lat2)\n",
    "        linkContent['lon2'].append(lon2)\n",
    "        linkContent['mag2'].append(G.nodes[e[1]]['mag'])\n",
    "        fz, dist = _objection_val(sol, e, key2id, geod)\n",
    "        linkContent['dist'].append(dist/1e3)\n",
    "        linkContent['azi'].append(fz)\n",
    "        bb, cc = G.edges[e]['B'], G.edges[e]['C'] % 360\n",
    "        cc, bb = _normalize_fz_dist(cc, bb)\n",
    "        linkContent['B'].append(bb)\n",
    "        linkContent['C'].append(cc)\n",
    "        linkContent['dB'].append(G.edges[e]['dB'])\n",
    "        linkContent['dC'].append(G.edges[e]['dC'] % 360)\n",
    "        linkContent['group'].append(G.nodes[e[0]]['group'])\n",
    "        linkContent['weight'].append(sol[-1])\n",
    "\n",
    "    def mergeCatalogue():\n",
    "        content_strs = ['id', 'time', 'lat', 'lon', 'mag', 'group']\n",
    "        content = {x: [] for x in content_strs}\n",
    "        dforg = pd.read_csv(os.path.join(fa.dir, \"catalog.csv\"))\n",
    "        dfcc = pd.read_csv(os.path.join(fa.dir, \"catalog-relocated.csv\"))\n",
    "        relocatedID = set(dfcc[\"id\"].to_list())\n",
    "        for r in dfcc.itertuples():\n",
    "            content['id'].append(r.id)\n",
    "            content['time'].append(r.time)\n",
    "            content['lat'].append(r.lat)\n",
    "            content['lon'].append(r.lon)\n",
    "            content['mag'].append(r.mag)\n",
    "            content['group'].append(r.group)\n",
    "        for r in dforg.itertuples():\n",
    "            if r.id not in relocatedID:\n",
    "                mw = mag2mw(r.mag, r.magType)\n",
    "                content['id'].append(r.id)\n",
    "                content['time'].append(r.time)\n",
    "                content['lat'].append(r.lat)\n",
    "                content['lon'].append(r.lon)\n",
    "                content['mag'].append(mw)\n",
    "                content['group'].append(-1)\n",
    "        pd.DataFrame(content).sort_values(by=[\"time\"]).to_csv(\n",
    "            os.path.join(fa.dir, \"catalog-merged.csv\"), index=False)\n",
    "\n",
    "    def _objection_val(sol, e: list, key2id: dict, geod):\n",
    "        id1, id2 = key2id[e[0]], key2id[e[1]]\n",
    "        lat1, lon1 = sol[2*id1], sol[2*id1+1]\n",
    "        lat2, lon2 = sol[2*id2], sol[2*id2+1]\n",
    "        fz, _, dist = geod.inv(lon1, lat1, lon2, lat2)\n",
    "        fz, dist = _normalize_fz_dist(fz, dist)\n",
    "        return fz, dist\n",
    "\n",
    "    def _normalize_fz_dist(fz: float, dist: float):\n",
    "        if dist < 0:\n",
    "            dist *= -1\n",
    "            fz += 180.0\n",
    "        fz %= 360\n",
    "        return fz, dist\n",
    "\n",
    "    def objectiveFuncFactory(masterID, masterLat, masterLon, Bs, Cs, dBs, dCs, key2id, es, weight=1.0):\n",
    "        # weight denotes relative contribution between `dist` and `azi`\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        _B = copy.copy(Bs)\n",
    "        _C = list(map(lambda x: x % 360, Cs))\n",
    "        _dB = copy.copy(dBs)\n",
    "        _dC = list(map(lambda x: x % 360, dCs))\n",
    "        for i in range(len(_B)):\n",
    "            _C[i], _B[i] = _normalize_fz_dist(_C[i], _B[i])\n",
    "            _dC[i], _dB[i] = _normalize_fz_dist(_dC[i], _dB[i])\n",
    "            if _dC[i] > 180:\n",
    "                _dC[i] = 360.0 - _dC[i]\n",
    "\n",
    "        _dB2 = [x**2 for x in _dB]\n",
    "        _dC2 = [x**2 for x in _dC]\n",
    "\n",
    "        def objective(x, grad):\n",
    "            # x = [lat_1, lon_1, lat_2, lon_2, ..., lat_n, lon_n, weight]\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"Use gradient free method!\")\n",
    "\n",
    "            y = 0.0\n",
    "            for i, e in enumerate(es):\n",
    "                fz, dist = _objection_val(x, e, key2id, geod)\n",
    "                y += np.abs((dist/1e3 - _B[i]) / _dB[i]) + \\\n",
    "                    x[-1] * np.abs(angularDiff(fz, _C[i]) / _dC[i])\n",
    "            return np.sqrt(y)\n",
    "\n",
    "        def masterLatitudeConstraint(x, grad):\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"Use gradient free method!\")\n",
    "            return x[2 * key2id[masterID]] - masterLat\n",
    "\n",
    "        def masterLongitudeConstraint(x, grad):\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"User gradient free method!\")\n",
    "            return x[2 * key2id[masterID] + 1] - masterLon\n",
    "\n",
    "        return objective, masterLatitudeConstraint, masterLongitudeConstraint\n",
    "\n",
    "    G = buildPairGraph()\n",
    "    G = traverseGraph(G, True)\n",
    "    optimizeLocation(G, relocateTwoWay=True, relocateGlobal=False)\n",
    "    mergeCatalogue()\n",
    "    \n",
    "# Create an instance of RelocationProcedure for your fault\n",
    "fault = RelocationProcedure(\"Fault1\")\n",
    "# Run cross-correlation\n",
    "crossCorrelate(fault)\n",
    "# Run optimization\n",
    "optimize(fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbc66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start from below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9257e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import operator\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque, namedtuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, wait\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nlopt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from obspy import read\n",
    "from obspy.imaging.beachball import beach\n",
    "from obspy.io.sac.util import SacIOError\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "from pyproj import Geod\n",
    "from scipy.optimize import curve_fit, least_squares, minimize\n",
    "\n",
    "from GetData import AbstractFaultProcess\n",
    "from utils import *\n",
    "\n",
    "def cc(id1, t1, lat1, lon1, mag1, id2, t2, lat2, lon2, mag2):\n",
    "    outlierCC = 0.70\n",
    "    outlierDt = 35\n",
    "    numCCToFit = 8\n",
    "    vgroup = 3.75\n",
    "    outlierDx = vgroup * outlierDt\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "\n",
    "    def badCoverage(azi, maxgap=120.0):  # exclude those with azimuth gap larger than `maxgap`\n",
    "        if len(azi) == 0:\n",
    "            return False\n",
    "        _azi = sorted([x % 360 for x in azi])\n",
    "        diff = [_azi[i+1] - _azi[i] for i in range(len(_azi) - 1)]\n",
    "        diff.append(360 - _azi[-1] + _azi[0])\n",
    "        return np.any(np.array(diff) > maxgap)\n",
    "\n",
    "    def residual_cosine(x, azi, dt):\n",
    "        return x[0] + x[1] * np.cos(np.deg2rad(azi - x[2])) - dt\n",
    "\n",
    "    def fit_cosine_ls_norm(azi, dx, cc, u0):\n",
    "        mask = np.logical_and(cc >= outlierCC, np.abs(dx) <= outlierDx)\n",
    "        _azi = azi[mask]\n",
    "        _dx = dx[mask]\n",
    "        if len(_azi) < numCCToFit or badCoverage(_azi):\n",
    "            return np.array([np.nan, np.nan, np.nan]), np.array([np.inf, np.inf, np.inf])\n",
    "\n",
    "        # initial solution affect the local minimum obtained\n",
    "        res = least_squares(residual_cosine, [0., u0, 0.], args=(\n",
    "            _azi, _dx), loss='soft_l1', jac='3-point')\n",
    "\n",
    "        # On way of uncertainty, but inappropriate since we use other forms of norm\n",
    "        # https://stackoverflow.com/a/21844726/8697614\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # pcov = cov * (res.fun ** 2).sum() / (len(_azi) - len(res.x)) # reduced chi-square distribution\n",
    "        # err = np.sqrt(np.diagonal(pcov))\n",
    "\n",
    "        # Another way of uncertainty, still assume your function value is chi-square\n",
    "        # https://stackoverflow.com/a/53489234/8697614, notice `dx[2] can exceed 360`\n",
    "        # J = res.jac\n",
    "        # cov = np.linalg.inv(J.T.dot(J))\n",
    "        # err2 = np.sqrt(np.diagonal(cov) * np.abs(res.cost))\n",
    "        # return res.x, err\n",
    "\n",
    "        # The third way: bootstrap\n",
    "        # This is what McGuire did, assuming 1s error in dt\n",
    "        # err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=1.0 * vgroup)\n",
    "        # we instead use fitting residual as the error for bootstrap\n",
    "        err = bootstrap_uncertainty(_azi, _dx, [0., u0, 0.], stderr=np.std(\n",
    "            residual_cosine(res.x, _azi, _dx)))\n",
    "        return res.x, err\n",
    "\n",
    "    def bootstrap_uncertainty(x, y, p0, numiters=400, stderr=3.75, nsigma=1.0):\n",
    "        x_cmt, x_dist, x_azi = [], [], []\n",
    "        n = len(y)\n",
    "        for i in range(numiters):\n",
    "            yy = y + np.random.default_rng().normal(0.0, stderr, n)\n",
    "            res = least_squares(residual_cosine, p0, args=(\n",
    "                x, yy), loss='soft_l1', jac='3-point')\n",
    "            x_cmt.append(res.x[0])\n",
    "            x_dist.append(res.x[1])\n",
    "            x_azi.append(res.x[2])\n",
    "        return nsigma*np.std(x_cmt), nsigma*np.std(x_dist), nsigma*np.std(x_azi)\n",
    "\n",
    "    fm1, fm2 = fm[str(id1)][\"np1\"], fm[str(id2)][\"np1\"]\n",
    "    wv1, wv2 = [], []\n",
    "    content = {x: [] for x in [\"azi\", \"dt\",\n",
    "                               \"cc\", \"net\", \"sta\", \"staLat\", \"staLon\"]}\n",
    "    nameBase = os.path.join(\n",
    "        ccDir, '-'.join(map(str, [id1, id2])))\n",
    "    dfStations = pd.read_csv(os.path.join(faultDir, \"stations.csv\"))\n",
    "    for rs in dfStations.itertuples():\n",
    "        waveName1 = str(id1) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        waveName2 = str(id2) + '-' + rs.net + '-' + rs.sta + '.sac'\n",
    "        wavePath1 = os.path.join(waveDir, waveName1)\n",
    "        wavePath2 = os.path.join(waveDir, waveName2)\n",
    "\n",
    "        if (\n",
    "            np.isnan(rs.dist) or\n",
    "            not os.path.isfile(wavePath1) or\n",
    "            not os.path.isfile(wavePath2) or\n",
    "            os.path.getsize(wavePath1) == 0 or\n",
    "            os.path.getsize(wavePath2) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        midLon, midLat = angularMean(\n",
    "            [lon1, lon2]), angularMean([lat1, lat2])\n",
    "        fz, _, _ = geod.inv(midLon, midLat, rs.lon,\n",
    "                            rs.lat)  # fz: (-180, 180)\n",
    "        try:\n",
    "            st1 = read(wavePath1, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath1)\n",
    "            continue\n",
    "        try:\n",
    "            st2 = read(wavePath2, format=\"SAC\")\n",
    "        except SacIOError:\n",
    "            os.remove(wavePath2)\n",
    "            continue\n",
    "\n",
    "        if st1[0].stats[\"sampling_rate\"] != st2[0].stats[\"sampling_rate\"]:\n",
    "            maxSamplingRate = max(st1[0].stats[\"sampling_rate\"], st2[0].stats[\"sampling_rate\"])\n",
    "            for ss in [st1, st2]:\n",
    "                if maxSamplingRate != ss[0].stats[\"sampling_rate\"]:\n",
    "                    ss.resample(maxSamplingRate, no_filter=True)\n",
    "\n",
    "        cc = correlate(\n",
    "            st1[0], st2[0], demean=True, normalize='naive',\n",
    "            shift=min(st1[0].stats.npts, st2[0].stats.npts, int(\n",
    "                np.round(outlierDt / st1[0].stats.delta * 2.0)))\n",
    "        )\n",
    "        # consider only positive cc\n",
    "        shift, value = xcorr_max(cc, abs_max=False)\n",
    "        dt = st1[0].stats.delta * shift\n",
    "        content['azi'].append(fz)\n",
    "        content['dt'].append(dt)\n",
    "        content['cc'].append(value)\n",
    "        content['net'].append(rs.net)\n",
    "        content['sta'].append(rs.sta)\n",
    "        content['staLat'].append(rs.lat)\n",
    "        content['staLon'].append(rs.lon)\n",
    "        wv1.append(st1[0].data)\n",
    "        wv2.append(st2[0].data)\n",
    "\n",
    "    pd.DataFrame(content).to_csv(nameBase + \".csv\", index=False)\n",
    "    if len(content[\"dt\"]) < numCCToFit:\n",
    "        return\n",
    "\n",
    "    yy = np.array(content['dt']) * vgroup\n",
    "    xx = np.array(content['azi'])\n",
    "    # initial guess of distance, important for least square fit\n",
    "    _, _, u0 = geod.inv(lon1, lat1, lon2, lat2)\n",
    "    popt, pcov = fit_cosine_ls_norm(\n",
    "        xx, yy, np.array(content['cc']), np.abs(u0 / 1e3))\n",
    "    A, B, C = popt\n",
    "    dA, dB, dC = pcov\n",
    "    highcc = list(filter(lambda x: np.abs(x) >= outlierCC, content['cc']))\n",
    "    summary = {\n",
    "        'A': popt[0], 'B': popt[1], 'C': popt[2],\n",
    "        'dA': pcov[0], 'dB': pcov[1], 'dC': pcov[2],\n",
    "        'numHighCC': len(highcc),\n",
    "    }\n",
    "    with open(nameBase + \".json\", \"w\") as fp:\n",
    "        json.dump(summary, fp, indent=4)\n",
    "\n",
    "    def plotCosineFit():\n",
    "        fig = plt.figure(figsize=(8, 10))\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[2, 1])\n",
    "        ax0 = fig.add_subplot(gs[0, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(\n",
    "            gs[1, 0], projection=ccrs.AzimuthalEquidistant(lon1, lat1))\n",
    "        ax2.stock_img()\n",
    "        ax2.scatter(lon1, lat1, s=20, marker=\"*\", c=\"orange\",\n",
    "                    transform=ccrs.PlateCarree(), zorder=2)\n",
    "\n",
    "        ax3 = fig.add_axes([0.55, 0.89, 0.1, 0.1])\n",
    "        ax3.set_aspect(\"equal\")\n",
    "        if np.isnan(fm1[0]):\n",
    "            bc1 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.75), width=0.4)\n",
    "        else:\n",
    "            bc1 = beach(fm1, facecolor=\"tab:blue\", xy=(0.5, 0.75), width=0.4)\n",
    "        if np.isnan(fm2[0]):\n",
    "            bc2 = beach([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], facecolor=\"white\",\n",
    "                        edgecolor=\"lightgray\", xy=(0.5, 0.25), width=0.4)\n",
    "        else:\n",
    "            bc2 = beach(fm2, facecolor=\"tab:red\", xy=(0.5, 0.25), width=0.4)\n",
    "        ax3.add_collection(bc1)\n",
    "        ax3.add_collection(bc2)\n",
    "        ax3.axis('off')\n",
    "\n",
    "        for i in range(len(wv1)):\n",
    "            # do not set `step=delta` which may cause length mismatch\n",
    "            y1 = np.arange(0.0, len(wv1[i])) * st1[0].stats.delta\n",
    "            y2 = np.arange(0.0, len(wv2[i])) * st2[0].stats.delta\n",
    "            w1 = wv1[i] - np.nanmean(wv1[i])\n",
    "            w2 = wv2[i] - np.nanmean(wv2[i])\n",
    "            w1 = w1 / np.nanmax(np.abs(w1)) * 8\n",
    "            w2 = w2 / np.nanmax(np.abs(w2)) * 8\n",
    "\n",
    "            isoutlier = not (np.abs(content[\"dt\"][i]) <= outlierDt and np.abs(\n",
    "                content[\"cc\"][i]) >= outlierCC)\n",
    "            linestyle = '--' if isoutlier else '-'\n",
    "            alpha = 0.2 if isoutlier else 1.0\n",
    "            fc = 'white' if isoutlier else 'tab:green'\n",
    "            ax0.plot(y1, w1 + content[\"azi\"][i], color=\"tab:blue\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax0.plot(y2, w2 + content[\"azi\"][i], color=\"tab:red\",\n",
    "                     linestyle=linestyle, alpha=alpha, clip_on=False)\n",
    "            ax1.scatter(content[\"dt\"][i], content[\"azi\"][i],\n",
    "                        fc=fc, s=15, marker=\"o\", ec=\"black\", zorder=2)\n",
    "\n",
    "            trajectorColor = \"gray\" if isoutlier else \"tab:pink\"\n",
    "            trajectorStyle = \"--\" if isoutlier else \"-\"\n",
    "            ax2.plot([lon1, content[\"staLon\"][i]], [lat1, content[\"staLat\"][i]],\n",
    "                     color=trajectorColor, transform=ccrs.Geodetic(), zorder=1, linestyle=trajectorStyle)\n",
    "\n",
    "        xx = np.arange(-180, 180, 1.0)\n",
    "        if np.isnan(A):\n",
    "            yy = np.array([np.nan for _ in xx])\n",
    "            fittingColor = \"gray\"\n",
    "        else:\n",
    "            yy = A + B * np.cos(np.deg2rad(xx - C))\n",
    "            yy /= vgroup\n",
    "            fittingColor = \"tab:orange\"\n",
    "\n",
    "        ax1.plot(yy, xx, c=fittingColor, zorder=1)\n",
    "        _b, _c = B, C\n",
    "        if _b < 0:\n",
    "            _b *= -1\n",
    "            _c += 180\n",
    "        _c %= 360\n",
    "        ax1.text(0.5, 1.05, \"$\\Delta x = \\;$\" + f\"{_b:.0f}\" + \"$\\;\\pm \\;$\" + f\"{dB:.0f}\" + \"$\\;\\mathrm{km}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax1.text(0.5, 1.12, r\"$\\theta = \\;$\" + f\"{_c:.1f}\" + \"$\\;\\pm \\;$\" + f\"{dC:.1f}\" + \"$^{\\circ}$\",\n",
    "                 ha='center', va='center', transform=ax1.transAxes, backgroundcolor=\"steelblue\", color=\"white\")\n",
    "        ax0.set_ylim([-180, 180])\n",
    "        ax1.set_ylim([-180, 180])\n",
    "        ax1.set_xlim([-outlierDt, outlierDt])\n",
    "        ax1.set_yticklabels([])\n",
    "        ax1.grid(True, which=\"major\", color='gray',\n",
    "                 linestyle='--', linewidth=1)\n",
    "        ax1.set_xlabel(\"$\\Delta T$ (second)\")\n",
    "        ax0.set_xlabel(\"Time (second)\")\n",
    "        ax0.set_ylabel(\"Azimuth (deg)\")\n",
    "        ax0.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "        l1, = ax0.plot([], [], color=\"tab:blue\")\n",
    "        l2, = ax0.plot([], [], color=\"tab:red\")\n",
    "        _tstr1 = datetime.fromisoformat(t1)\n",
    "        _tstr1 = _tstr1.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag1:.1f}')\n",
    "        _tstr2 = datetime.fromisoformat(t2)\n",
    "        _tstr2 = _tstr2.strftime(\"%Y-%m-%d %H:%M:%S\"+f'  M{mag2:.1f}')\n",
    "        ax0.legend([l1, l2], [_tstr1, _tstr2],\n",
    "                   loc=\"upper center\", bbox_to_anchor=(0.5, 1.18))\n",
    "\n",
    "        l1, = ax1.plot([], [], color=\"tab:orange\")\n",
    "        l2 = ax0.scatter([], [], fc='tab:green', s=15, marker=\"o\", ec=\"black\")\n",
    "        l3 = ax0.scatter([], [], fc='white', s=15, marker=\"o\", ec=\"black\")\n",
    "        ax1.legend([l1, l2, l3], [\"Cosine Fitting\", \"Observation (cc $\\geq \\;$\" +\n",
    "                                  f\"{outlierCC:.2f})\", f\"Observation (cc $<$ {outlierCC:.2f})\"], loc=\"lower center\", bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "        l1, = ax2.plot([], [], color=\"tab:pink\", linestyle=\"-\")\n",
    "        l2, = ax2.plot([], [], color=\"gray\", linestyle=\"--\")\n",
    "        l3 = ax2.scatter([], [], s=20, marker=\"*\", c=\"orange\")\n",
    "        ax2.legend([l1, l2, l3], [\"$\\mathrm{cc} \\geq \\;$\"+f\"{outlierCC:.2f}\", \"$\\mathrm{cc} <\\;$\" +\n",
    "                                  f\"{outlierCC:.2f}\"], loc=\"lower left\", bbox_to_anchor=(-0.60, 0.0))\n",
    "        fig.savefig(nameBase + \".pdf\")\n",
    "        print(f\"Saved {os.path.basename(nameBase)}.\")\n",
    "        #plt.close(fig)\n",
    "\n",
    "    plotCosineFit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f9dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class RelocationProcedure(AbstractFaultProcess):\n",
    "waveDir = r\"C:\\Users\\student\\Downloads\\OTF2021-1.0.1\\cross-correl-sample\\six_earthquakes\\Fault1\\waves\"\n",
    "faultDir = r\"C:\\Users\\student\\Downloads\\OTF2021-1.0.1\\cross-correl-sample\\six_earthquakes\\Fault1\"\n",
    "ccDir = r\"C:\\Users\\student\\Downloads\\OTF2021-1.0.1\\cross-correl-sample\\six_earthquakes\\Fault1\\cc\"\n",
    "with open(os.path.join(faultDir, \"mt.json\")) as fp:\n",
    "            fm = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec8eea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us700054 2020-09-06 08:24:15 7.849 -37.106 4.479 us700068 2020-09-07 00:46:32 7.811 -37.024 4.479\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def crossCorrelate():\n",
    "    dfPairss = pd.read_csv(os.path.join(faultDir, \"catalog-pair.csv\"))\n",
    "    total_pairs = dfPairss.shape[0]\n",
    "    processed_pairs = 0\n",
    "\n",
    "    ccFiles = [os.path.join(ccDir, x) for x in os.listdir(ccDir)]\n",
    "    for f in ccFiles:\n",
    "        os.remove(f)\n",
    "\n",
    "    for r in dfPairss.itertuples(index=True):\n",
    "        print( r.id1, r.t1, r.lat1, r.lon1, r.mag1, r.id2, r.t2, r.lat2, r.lon2, r.mag2)\n",
    "        cc(r.id1, r.t1, r.lat1,r.lon1, r.mag1, r.id2, r.t2, r.lat2, r.lon2, r.mag2)\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "crossCorrelate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c8dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize():\n",
    "\n",
    "    def formEdge(x: dict, linkNum: int = 8):\n",
    "        return \\\n",
    "            (not np.isnan(x['B'])) and \\\n",
    "            (not np.isinf(x['dB'])) and \\\n",
    "            x['numHighCC'] >= linkNum\n",
    "\n",
    "    def buildPairGraph():\n",
    "        dfPairs = pd.read_csv(os.path.join(faultDir, \"catalog-pair.csv\"))\n",
    "        files = (x for x in os.listdir(ccDir) if x.endswith(\".json\"))\n",
    "        pairkey2row = {(r.id1, r.id2): r for r in dfPairs.itertuples()}\n",
    "\n",
    "        G = nx.Graph()\n",
    "        for file in files:\n",
    "            with open(os.path.join(ccDir, file), \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "                if formEdge(data):\n",
    "                    info = os.path.splitext(file)[0].split('-')\n",
    "                    id1, id2 = map(str, info)\n",
    "                    r = pairkey2row.get((id1, id2), None)\n",
    "                    if not r:\n",
    "                        continue\n",
    "                    if id1 != r.id1 or id2 != r.id2:\n",
    "                        raise ValueError(\"pair id mismatch.\")\n",
    "\n",
    "                    # duplicate hashing is ignored\n",
    "                    G.add_node(id1,\n",
    "                               lat=r.lat1, lon=r.lon1, mag=r.mag1,\n",
    "                               t=r.t1, group=-1)\n",
    "                    G.add_node(id2,\n",
    "                               lat=r.lat2, lon=r.lon2, mag=r.mag2,\n",
    "                               t=r.t2, group=-1)\n",
    "                    G.add_edge(id1, id2,\n",
    "                               B=data['B'], C=data['C'],\n",
    "                               dB=data['dB'], dC=data['dC'])\n",
    "        return G\n",
    "\n",
    "    def traverseGraph(G, relocateOneWay=True):\n",
    "        # BFS, relocate one group of events by single reference abiding shortest path\n",
    "        # This, however, does not account for discrepancy with direct observation and\n",
    "        # new locations. Using optimization is perferred, see below.\n",
    "\n",
    "        # If not relocate, it aims to idenfity groups and label accordingly.\n",
    "        def traverse(n):\n",
    "            queueSet.add(n)\n",
    "            for k in G.adj[n].keys():\n",
    "                if (not visited[k]) and (k not in queueSet):\n",
    "                    if relocateOneWay:\n",
    "                        B, C = G.adj[n][k]['B'], G.adj[n][k]['C']\n",
    "                        # all pairs id1 is more recent than id2\n",
    "                        C = C if IDTimePair[n] > IDTimePair[k] else C + 180.0  # only change `B` or `C`, not both\n",
    "                        lat1, lon1 = G.nodes.data(\n",
    "                            'lat')[n], G.nodes.data('lon')[n]\n",
    "                        lon2, lat2, _ = geod.fwd(lon1, lat1, C, B*1e3)\n",
    "                        G.nodes[k].update(lat=lat2, lon=lon2)\n",
    "                    visited[k] = True\n",
    "                    IDTimePairDynamic.pop(k, None)\n",
    "                    G.nodes[k].update(group=groupid)\n",
    "                    Q.append(k)\n",
    "                    queueSet.add(k)\n",
    "\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        Q = deque()\n",
    "        queueSet = set()\n",
    "        visited = dict(G.nodes.data('visited'))\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "        IDTimePairDynamic = dict(G.nodes.data(\"t\"))\n",
    "        groupid = 1\n",
    "\n",
    "        while count := sum([1 for (k, v) in visited.items() if not v]) > 0:\n",
    "            # use newest event as reference\n",
    "            startID = max(IDTimePairDynamic, key=IDTimePairDynamic.get)\n",
    "            Q.append(startID)\n",
    "            visited[startID] = True\n",
    "            IDTimePairDynamic.pop(startID)\n",
    "            G.nodes[startID].update(group=groupid)\n",
    "            while Q:\n",
    "                traverse(Q.popleft())\n",
    "            groupid += 1\n",
    "        return G\n",
    "\n",
    "    def traverseOptimizeGraph(G):\n",
    "        Q = deque()\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        optimized = dict(G.nodes.data('visited'))\n",
    "        queueSet = set()\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "        IDTimePair2 = dict(G.nodes.data(\"t\"))\n",
    "\n",
    "        def optimizeNodeLocation(node):\n",
    "\n",
    "            def objective(x, lons, lats, B, C, dB, dC):\n",
    "                res = 0.0\n",
    "                for i in range(len(lons)):\n",
    "                    fz, _, dist = geod.inv(x[0], x[1], lons[i], lats[i])\n",
    "                    fz, dist = _normalize_fz_dist(fz, dist)\n",
    "                    res += np.abs(angularDiff(fz, C[i]) / dC[i]) + \\\n",
    "                        np.abs((dist/1e3 - B[i]) / dB[i])\n",
    "                return res\n",
    "\n",
    "            ks = G.adj[node].keys()\n",
    "            queueSet.add(node)\n",
    "            IDTimePair.pop(node, None)\n",
    "            for k in ks:\n",
    "                if (not optimized[k]) and (k not in queueSet):\n",
    "                    Q.append(k)\n",
    "                    queueSet.add(k)\n",
    "                    IDTimePair.pop(k, None)\n",
    "\n",
    "            lons = np.array([G.nodes.data('lon')[k] for k in ks])\n",
    "            lats = np.array([G.nodes.data('lat')[k] for k in ks])\n",
    "            x0 = np.array([angularMean(lons), angularMean(lats)])\n",
    "            Bs = np.array([G.adj[node][k]['B'] for k in ks])\n",
    "            Cs = np.array([G.adj[node][k]['C'] for k in ks])\n",
    "            dBs = np.array([G.adj[node][k]['dB'] for k in ks])\n",
    "            dCs = np.array([G.adj[node][k]['dC'] for k in ks])\n",
    "            for i, k in enumerate(ks):\n",
    "                # all pairs id1 is more recent than id2\n",
    "                if IDTimePair2[node] < IDTimePair2[k]:  # correction for direction\n",
    "                    Cs[i] += 180.0\n",
    "                if Bs[i] < 0:  # correction for negative distance\n",
    "                    Bs[i] *= -1\n",
    "                    Cs[i] += 180.0\n",
    "                Cs[i] %= 360  # correction for azimuth angle between [0, 360)\n",
    "            res = minimize(objective, x0, args=(\n",
    "                lons, lats, Bs, Cs, dBs, dCs), method=\"Nelder-Mead\")\n",
    "            G.nodes[node].update(lat=res.x[1], lon=res.x[0])\n",
    "            optimized[node] = True\n",
    "\n",
    "        while count := sum([1 for (k, v) in optimized.items() if not v]) > 0:\n",
    "            # startID = min([k for (k, v) in optimized.items() if not v])\n",
    "            startID = max(IDTimePair, key=IDTimePair.get)\n",
    "            Q.append(startID)\n",
    "            IDTimePair.pop(startID)\n",
    "            while Q:\n",
    "                optimizeNodeLocation(Q.popleft())\n",
    "        return G\n",
    "\n",
    "    def optimizeLocation(G, relocateTwoWay=True, relocateTwoWayIter=5, relocateGlobal=True):\n",
    "        if relocateTwoWay:\n",
    "            for _ in range(relocateTwoWayIter):\n",
    "                traverseOptimizeGraph(G)\n",
    "\n",
    "        data = {k: G.nodes[k] for k in G.nodes}\n",
    "        uniqueGroupID = set([data[k]['group'] for k in data.keys()])\n",
    "        linkContent = {x: [] for x in [\n",
    "            \"id1\", \"t1\", \"lat1\", \"lon1\", \"mag1\",\n",
    "            \"id2\", \"t2\", \"lat2\", \"lon2\", \"mag2\",\n",
    "            \"dist\", \"B\", \"dB\",\n",
    "            \"azi\", \"C\", \"dC\",\n",
    "            \"group\", \"weight\",\n",
    "        ]}\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        IDTimePair = dict(G.nodes.data(\"t\"))\n",
    "\n",
    "        for gid in uniqueGroupID:\n",
    "            es = [x for x in G.edges if data[x[0]]['group'] == gid]\n",
    "            es.sort(key=lambda x: (IDTimePair[x[0]], IDTimePair[x[1]]), reverse=True)\n",
    "            pks = list(set(itertools.chain(*es)))\n",
    "            pks.sort(key=lambda x: IDTimePair[x], reverse=True)\n",
    "            id2t = {k: IDTimePair[k] for k in pks}\n",
    "            masterId = max(id2t, key=id2t.get)\n",
    "            masterLat, masterLon = data[masterId]['lat'], data[masterId]['lon']\n",
    "            key2id = {pks[i]: i for i in range(len(pks))}\n",
    "            Bs = [G.edges[x]['B'] for x in es]\n",
    "            Cs = [G.edges[x]['C'] for x in es]\n",
    "            dBs = [G.edges[x]['dB'] for x in es]\n",
    "            dCs = [G.edges[x]['dC'] for x in es]\n",
    "            lat0 = np.array([data[x]['lat'] for x in pks])\n",
    "            lon0 = np.array([data[x]['lon'] for x in pks])\n",
    "            u0 = [[lat0[i], lon0[i]] for i in range(len(pks))]\n",
    "            u0 = list(itertools.chain(*u0))\n",
    "            u0.append(1)  # initial weight\n",
    "\n",
    "            if not relocateGlobal:\n",
    "                x = u0\n",
    "            else:\n",
    "                avglat0, avglon0 = angularMean(lat0), angularMean(lon0)\n",
    "                searchRange = 10.0  # within 10 degree, should be sufficient\n",
    "                # not rigorously right,\n",
    "                # but we don't have events at polar region or date changing line\n",
    "                lblat = np.clip(lat0 - searchRange, -90, 90)\n",
    "                ublat = np.clip(lat0 + searchRange, -90, 90)\n",
    "                lblon = np.clip(lon0 - searchRange, -180, 180)\n",
    "                ublon = np.clip(lon0 + searchRange, -180, 180)\n",
    "\n",
    "                lb = [[x, y] for x, y in zip(lblat, lblon)]\n",
    "                lb = list(itertools.chain(*lb))\n",
    "                lb.append(0.2)  # lower bound for relative weight\n",
    "                ub = [[x, y] for x, y in zip(ublat, ublon)]\n",
    "                ub = list(itertools.chain(*ub))\n",
    "                ub.append(2)  # upper bound for relative weight\n",
    "\n",
    "                objective, masterLatitudeConstraint, masterLongitudeConstraint = \\\n",
    "                    objectiveFuncFactory(\n",
    "                        masterId, masterLat, masterLon, Bs, Cs, dBs, dCs, key2id, es)\n",
    "\n",
    "                opt = nlopt.opt(nlopt.LN_COBYLA, 2*len(pks)+1)\n",
    "                opt.set_min_objective(objective)\n",
    "                opt.add_equality_constraint(masterLatitudeConstraint, 1e-6)\n",
    "                opt.add_equality_constraint(masterLongitudeConstraint, 1e-6)\n",
    "                opt.set_lower_bounds(lb)\n",
    "                opt.set_upper_bounds(ub)\n",
    "                opt.set_xtol_rel(1e-6)\n",
    "                x = opt.optimize(u0)\n",
    "\n",
    "            for e in es:\n",
    "                _add_link_content(linkContent, G, e, x, key2id, geod, IDTimePair)\n",
    "\n",
    "        pd.DataFrame(linkContent).to_csv(os.path.join(\n",
    "            faultDir, \"catalog-link.csv\"), index=False)\n",
    "        content = {'id': [], 'time': [], 'lat': [],\n",
    "                   'lon': [], 'mag': [], 'group': []}\n",
    "        content['id'].extend(linkContent['id1'])\n",
    "        content['id'].extend(linkContent['id2'])\n",
    "        content['time'].extend(linkContent['t1'])\n",
    "        content['time'].extend(linkContent['t2'])\n",
    "        content['lat'].extend(linkContent['lat1'])\n",
    "        content['lat'].extend(linkContent['lat2'])\n",
    "        content['lon'].extend(linkContent['lon1'])\n",
    "        content['lon'].extend(linkContent['lon2'])\n",
    "        content['mag'].extend(linkContent['mag1'])\n",
    "        content['mag'].extend(linkContent['mag2'])\n",
    "        content['group'].extend(linkContent['group'])\n",
    "        content['group'].extend(linkContent['group'])\n",
    "        pd.DataFrame(content).drop_duplicates(subset=['id'], keep='last').sort_values(\n",
    "            by=\"time\").to_csv(os.path.join(faultDir, \"catalog-relocated.csv\"), index=False)\n",
    "\n",
    "    def _add_link_content(linkContent, G, e, sol, key2id, geod, IDTimePair):\n",
    "        # repeated code\n",
    "        # all pairs id1 is more recent than id2\n",
    "        if IDTimePair[e[0]] < IDTimePair[e[1]]:\n",
    "            e = (e[1], e[0])\n",
    "        id1, id2 = key2id[e[0]], key2id[e[1]]\n",
    "\n",
    "        lat1, lon1 = sol[2*id1], sol[2*id1+1]\n",
    "        lat2, lon2 = sol[2*id2], sol[2*id2+1]\n",
    "\n",
    "        linkContent['id1'].append(e[0])\n",
    "        linkContent['t1'].append(G.nodes[e[0]]['t'])\n",
    "        linkContent['lat1'].append(lat1)\n",
    "        linkContent['lon1'].append(lon1)\n",
    "        linkContent['mag1'].append(G.nodes[e[0]]['mag'])\n",
    "        linkContent['id2'].append(e[1])\n",
    "        linkContent['t2'].append(G.nodes[e[1]]['t'])\n",
    "        linkContent['lat2'].append(lat2)\n",
    "        linkContent['lon2'].append(lon2)\n",
    "        linkContent['mag2'].append(G.nodes[e[1]]['mag'])\n",
    "        fz, dist = _objection_val(sol, e, key2id, geod)\n",
    "        linkContent['dist'].append(dist/1e3)\n",
    "        linkContent['azi'].append(fz)\n",
    "        bb, cc = G.edges[e]['B'], G.edges[e]['C'] % 360\n",
    "        cc, bb = _normalize_fz_dist(cc, bb)\n",
    "        linkContent['B'].append(bb)\n",
    "        linkContent['C'].append(cc)\n",
    "        linkContent['dB'].append(G.edges[e]['dB'])\n",
    "        linkContent['dC'].append(G.edges[e]['dC'] % 360)\n",
    "        linkContent['group'].append(G.nodes[e[0]]['group'])\n",
    "        linkContent['weight'].append(sol[-1])\n",
    "\n",
    "    def mergeCatalogue():\n",
    "        content_strs = ['id', 'time', 'lat', 'lon', 'mag', 'group']\n",
    "        content = {x: [] for x in content_strs}\n",
    "        dforg = pd.read_csv(os.path.join(faultDir, \"catalog.csv\"))\n",
    "        dfcc = pd.read_csv(os.path.join(faultDir, \"catalog-relocated.csv\"))\n",
    "        relocatedID = set(dfcc[\"id\"].to_list())\n",
    "        for r in dfcc.itertuples():\n",
    "            content['id'].append(r.id)\n",
    "            content['time'].append(r.time)\n",
    "            content['lat'].append(r.lat)\n",
    "            content['lon'].append(r.lon)\n",
    "            content['mag'].append(r.mag)\n",
    "            content['group'].append(r.group)\n",
    "        for r in dforg.itertuples():\n",
    "            if r.id not in relocatedID:\n",
    "                mw = mag2mw(r.mag, r.magType)\n",
    "                content['id'].append(r.id)\n",
    "                content['time'].append(r.time)\n",
    "                content['lat'].append(r.lat)\n",
    "                content['lon'].append(r.lon)\n",
    "                content['mag'].append(mw)\n",
    "                content['group'].append(-1)\n",
    "        pd.DataFrame(content).sort_values(by=[\"time\"]).to_csv(\n",
    "            os.path.join(faultDir, \"catalog-merged.csv\"), index=False)\n",
    "\n",
    "    def _objection_val(sol, e: list, key2id: dict, geod):\n",
    "        id1, id2 = key2id[e[0]], key2id[e[1]]\n",
    "        lat1, lon1 = sol[2*id1], sol[2*id1+1]\n",
    "        lat2, lon2 = sol[2*id2], sol[2*id2+1]\n",
    "        fz, _, dist = geod.inv(lon1, lat1, lon2, lat2)\n",
    "        fz, dist = _normalize_fz_dist(fz, dist)\n",
    "        return fz, dist\n",
    "\n",
    "    def _normalize_fz_dist(fz: float, dist: float):\n",
    "        if dist < 0:\n",
    "            dist *= -1\n",
    "            fz += 180.0\n",
    "        fz %= 360\n",
    "        return fz, dist\n",
    "\n",
    "    def objectiveFuncFactory(masterID, masterLat, masterLon, Bs, Cs, dBs, dCs, key2id, es, weight=1.0):\n",
    "        # weight denotes relative contribution between `dist` and `azi`\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        _B = copy.copy(Bs)\n",
    "        _C = list(map(lambda x: x % 360, Cs))\n",
    "        _dB = copy.copy(dBs)\n",
    "        _dC = list(map(lambda x: x % 360, dCs))\n",
    "        for i in range(len(_B)):\n",
    "            _C[i], _B[i] = _normalize_fz_dist(_C[i], _B[i])\n",
    "            _dC[i], _dB[i] = _normalize_fz_dist(_dC[i], _dB[i])\n",
    "            if _dC[i] > 180:\n",
    "                _dC[i] = 360.0 - _dC[i]\n",
    "\n",
    "        _dB2 = [x**2 for x in _dB]\n",
    "        _dC2 = [x**2 for x in _dC]\n",
    "\n",
    "        def objective(x, grad):\n",
    "            # x = [lat_1, lon_1, lat_2, lon_2, ..., lat_n, lon_n, weight]\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"Use gradient free method!\")\n",
    "\n",
    "            y = 0.0\n",
    "            for i, e in enumerate(es):\n",
    "                fz, dist = _objection_val(x, e, key2id, geod)\n",
    "                y += np.abs((dist/1e3 - _B[i]) / _dB[i]) + \\\n",
    "                    x[-1] * np.abs(angularDiff(fz, _C[i]) / _dC[i])\n",
    "            return np.sqrt(y)\n",
    "\n",
    "        def masterLatitudeConstraint(x, grad):\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"Use gradient free method!\")\n",
    "            return x[2 * key2id[masterID]] - masterLat\n",
    "\n",
    "        def masterLongitudeConstraint(x, grad):\n",
    "            if grad.size > 0:\n",
    "                raise RuntimeError(\"User gradient free method!\")\n",
    "            return x[2 * key2id[masterID] + 1] - masterLon\n",
    "\n",
    "        return objective, masterLatitudeConstraint, masterLongitudeConstraint\n",
    "\n",
    "    G = buildPairGraph()\n",
    "    G = traverseGraph(G, True)\n",
    "    optimizeLocation(G, relocateTwoWay=True, relocateGlobal=False)\n",
    "    mergeCatalogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb860f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\AppData\\Local\\Temp\\ipykernel_86068\\1343431507.py:98: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  res += np.abs(angularDiff(fz, C[i]) / dC[i]) + \\\n",
      "C:\\Users\\student\\AppData\\Local\\Temp\\ipykernel_86068\\1343431507.py:99: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  np.abs((dist/1e3 - B[i]) / dB[i])\n",
      "d:\\Users\\student\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:863: RuntimeWarning: invalid value encountered in subtract\n",
      "  np.max(np.abs(fsim[0] - fsim[1:])) <= fatol):\n",
      "C:\\Users\\student\\AppData\\Local\\Temp\\ipykernel_86068\\1343431507.py:99: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  np.abs((dist/1e3 - B[i]) / dB[i])\n"
     ]
    }
   ],
   "source": [
    "optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26281b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
